{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QihwF1WmKrkP",
        "outputId": "27a0d5ae-0e80-448d-e509-97c8a8087cf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arch in /usr/local/lib/python3.10/dist-packages (6.1.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from arch) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from arch) (1.10.1)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.10/dist-packages (from arch) (1.5.3)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.10/dist-packages (from arch) (0.13.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->arch) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->arch) (2022.7.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.12->arch) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-4mVanEYKiyM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import yfinance as yf\n",
        "import arch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx9VVZHIKyNd",
        "outputId": "0923cbc3-c2fb-4138-c915-046b09f0dbdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/My Drive/thesis/3rd_presentation')\n",
        "df = pd.read_csv('non_nan_4.csv')"
      ],
      "metadata": {
        "id": "3YG_cfkhK2N7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "GgKYgibMK6W2",
        "outputId": "e8dc9992-6620-40b7-b48f-1b6078215660"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Date  unemployment_rate      CPI  treasury_yield  GDP_growth  \\\n",
              "0    2000-02-01                4.1  170.000        6.661000    0.496560   \n",
              "1    2000-03-01                4.0  171.000        6.519500    0.511258   \n",
              "2    2000-04-01                3.8  170.900        6.256522    1.327803   \n",
              "3    2000-05-01                4.0  171.200        5.990526   -0.181797   \n",
              "4    2000-06-01                4.0  172.200        6.440455    0.305565   \n",
              "..          ...                ...      ...             ...         ...   \n",
              "275  2023-01-01                3.4  300.536        3.616190    0.390254   \n",
              "276  2023-02-01                3.6  301.648        3.531500   -0.442183   \n",
              "277  2023-03-01                3.5  301.808        3.746842   -0.442183   \n",
              "278  2023-04-01                3.4  302.918        3.663043   -0.442183   \n",
              "279  2023-05-01                3.7  304.127        3.460000   -0.442183   \n",
              "\n",
              "     SP500_return        AZN        BMY        JNJ        LLY        MRK  \\\n",
              "0       -1.522563 -12.828964 -13.228004 -16.339821 -11.121498 -21.701151   \n",
              "1        9.413333  22.264136  -0.218329  -2.079067   5.804243   0.913712   \n",
              "2       -3.266805   5.567379  -8.205683  17.437698  23.153694  12.400712   \n",
              "3       -1.572223  -0.148357   5.395746   8.484832  -1.296597   7.374072   \n",
              "4        1.728613  10.549735   5.788826  14.239888  31.641749   3.078671   \n",
              "..            ...        ...        ...        ...        ...        ...   \n",
              "275      6.776820  -3.584079   0.972908  -7.489384  -5.928822  -2.549213   \n",
              "276     -2.514271  -0.290649  -4.328217  -6.217115  -9.568502  -1.089288   \n",
              "277      3.313488   8.035329   0.507544   1.862736  10.703390   0.141189   \n",
              "278      1.985238   5.489119  -3.664707   5.612908  15.269915   9.289214   \n",
              "279      0.461619  -0.191204  -2.695194  -5.277949   8.487855  -4.382088   \n",
              "\n",
              "           NVO        NVS        PFE        ROG  inflation_change  \\\n",
              "0     2.220031   3.838386 -11.226228  54.440789             1.000   \n",
              "1     8.390897   6.420237  14.101954   6.922258             1.000   \n",
              "2    -0.097663   2.559423  15.213674   7.370518            -0.100   \n",
              "3    20.863985   5.169310   5.638019  -8.163265             0.300   \n",
              "4     2.813690   8.474599   8.076012  13.131313             1.000   \n",
              "..         ...        ...        ...        ...               ...   \n",
              "275   2.541749  -0.110227 -13.817335  16.968326             1.546   \n",
              "276   1.592445  -7.172811  -7.286115   5.451681             1.112   \n",
              "277  12.873250   9.367574   0.566924  11.025813             0.160   \n",
              "278   5.836894  16.334413  -4.681371  -1.517467             1.110   \n",
              "279  -3.967915  -6.161645  -2.237080  -2.162160             0.100   \n",
              "\n",
              "     unemp_change  treasury_yield_change  \n",
              "0            -0.1              -0.141500  \n",
              "1            -0.1              -0.141500  \n",
              "2            -0.2              -0.262978  \n",
              "3             0.2              -0.265995  \n",
              "4             0.0               0.449928  \n",
              "..            ...                    ...  \n",
              "275          -0.1              -0.274810  \n",
              "276           0.2              -0.084690  \n",
              "277          -0.1               0.215342  \n",
              "278          -0.1              -0.083799  \n",
              "279           0.3              -0.203043  \n",
              "\n",
              "[280 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0af27ded-39b0-4a03-86a8-6894f34cf3d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>unemployment_rate</th>\n",
              "      <th>CPI</th>\n",
              "      <th>treasury_yield</th>\n",
              "      <th>GDP_growth</th>\n",
              "      <th>SP500_return</th>\n",
              "      <th>AZN</th>\n",
              "      <th>BMY</th>\n",
              "      <th>JNJ</th>\n",
              "      <th>LLY</th>\n",
              "      <th>MRK</th>\n",
              "      <th>NVO</th>\n",
              "      <th>NVS</th>\n",
              "      <th>PFE</th>\n",
              "      <th>ROG</th>\n",
              "      <th>inflation_change</th>\n",
              "      <th>unemp_change</th>\n",
              "      <th>treasury_yield_change</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-02-01</td>\n",
              "      <td>4.1</td>\n",
              "      <td>170.000</td>\n",
              "      <td>6.661000</td>\n",
              "      <td>0.496560</td>\n",
              "      <td>-1.522563</td>\n",
              "      <td>-12.828964</td>\n",
              "      <td>-13.228004</td>\n",
              "      <td>-16.339821</td>\n",
              "      <td>-11.121498</td>\n",
              "      <td>-21.701151</td>\n",
              "      <td>2.220031</td>\n",
              "      <td>3.838386</td>\n",
              "      <td>-11.226228</td>\n",
              "      <td>54.440789</td>\n",
              "      <td>1.000</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-03-01</td>\n",
              "      <td>4.0</td>\n",
              "      <td>171.000</td>\n",
              "      <td>6.519500</td>\n",
              "      <td>0.511258</td>\n",
              "      <td>9.413333</td>\n",
              "      <td>22.264136</td>\n",
              "      <td>-0.218329</td>\n",
              "      <td>-2.079067</td>\n",
              "      <td>5.804243</td>\n",
              "      <td>0.913712</td>\n",
              "      <td>8.390897</td>\n",
              "      <td>6.420237</td>\n",
              "      <td>14.101954</td>\n",
              "      <td>6.922258</td>\n",
              "      <td>1.000</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-04-01</td>\n",
              "      <td>3.8</td>\n",
              "      <td>170.900</td>\n",
              "      <td>6.256522</td>\n",
              "      <td>1.327803</td>\n",
              "      <td>-3.266805</td>\n",
              "      <td>5.567379</td>\n",
              "      <td>-8.205683</td>\n",
              "      <td>17.437698</td>\n",
              "      <td>23.153694</td>\n",
              "      <td>12.400712</td>\n",
              "      <td>-0.097663</td>\n",
              "      <td>2.559423</td>\n",
              "      <td>15.213674</td>\n",
              "      <td>7.370518</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>-0.262978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-05-01</td>\n",
              "      <td>4.0</td>\n",
              "      <td>171.200</td>\n",
              "      <td>5.990526</td>\n",
              "      <td>-0.181797</td>\n",
              "      <td>-1.572223</td>\n",
              "      <td>-0.148357</td>\n",
              "      <td>5.395746</td>\n",
              "      <td>8.484832</td>\n",
              "      <td>-1.296597</td>\n",
              "      <td>7.374072</td>\n",
              "      <td>20.863985</td>\n",
              "      <td>5.169310</td>\n",
              "      <td>5.638019</td>\n",
              "      <td>-8.163265</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.265995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-06-01</td>\n",
              "      <td>4.0</td>\n",
              "      <td>172.200</td>\n",
              "      <td>6.440455</td>\n",
              "      <td>0.305565</td>\n",
              "      <td>1.728613</td>\n",
              "      <td>10.549735</td>\n",
              "      <td>5.788826</td>\n",
              "      <td>14.239888</td>\n",
              "      <td>31.641749</td>\n",
              "      <td>3.078671</td>\n",
              "      <td>2.813690</td>\n",
              "      <td>8.474599</td>\n",
              "      <td>8.076012</td>\n",
              "      <td>13.131313</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.449928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>2023-01-01</td>\n",
              "      <td>3.4</td>\n",
              "      <td>300.536</td>\n",
              "      <td>3.616190</td>\n",
              "      <td>0.390254</td>\n",
              "      <td>6.776820</td>\n",
              "      <td>-3.584079</td>\n",
              "      <td>0.972908</td>\n",
              "      <td>-7.489384</td>\n",
              "      <td>-5.928822</td>\n",
              "      <td>-2.549213</td>\n",
              "      <td>2.541749</td>\n",
              "      <td>-0.110227</td>\n",
              "      <td>-13.817335</td>\n",
              "      <td>16.968326</td>\n",
              "      <td>1.546</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.274810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>2023-02-01</td>\n",
              "      <td>3.6</td>\n",
              "      <td>301.648</td>\n",
              "      <td>3.531500</td>\n",
              "      <td>-0.442183</td>\n",
              "      <td>-2.514271</td>\n",
              "      <td>-0.290649</td>\n",
              "      <td>-4.328217</td>\n",
              "      <td>-6.217115</td>\n",
              "      <td>-9.568502</td>\n",
              "      <td>-1.089288</td>\n",
              "      <td>1.592445</td>\n",
              "      <td>-7.172811</td>\n",
              "      <td>-7.286115</td>\n",
              "      <td>5.451681</td>\n",
              "      <td>1.112</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.084690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>2023-03-01</td>\n",
              "      <td>3.5</td>\n",
              "      <td>301.808</td>\n",
              "      <td>3.746842</td>\n",
              "      <td>-0.442183</td>\n",
              "      <td>3.313488</td>\n",
              "      <td>8.035329</td>\n",
              "      <td>0.507544</td>\n",
              "      <td>1.862736</td>\n",
              "      <td>10.703390</td>\n",
              "      <td>0.141189</td>\n",
              "      <td>12.873250</td>\n",
              "      <td>9.367574</td>\n",
              "      <td>0.566924</td>\n",
              "      <td>11.025813</td>\n",
              "      <td>0.160</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.215342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>2023-04-01</td>\n",
              "      <td>3.4</td>\n",
              "      <td>302.918</td>\n",
              "      <td>3.663043</td>\n",
              "      <td>-0.442183</td>\n",
              "      <td>1.985238</td>\n",
              "      <td>5.489119</td>\n",
              "      <td>-3.664707</td>\n",
              "      <td>5.612908</td>\n",
              "      <td>15.269915</td>\n",
              "      <td>9.289214</td>\n",
              "      <td>5.836894</td>\n",
              "      <td>16.334413</td>\n",
              "      <td>-4.681371</td>\n",
              "      <td>-1.517467</td>\n",
              "      <td>1.110</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.083799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>2023-05-01</td>\n",
              "      <td>3.7</td>\n",
              "      <td>304.127</td>\n",
              "      <td>3.460000</td>\n",
              "      <td>-0.442183</td>\n",
              "      <td>0.461619</td>\n",
              "      <td>-0.191204</td>\n",
              "      <td>-2.695194</td>\n",
              "      <td>-5.277949</td>\n",
              "      <td>8.487855</td>\n",
              "      <td>-4.382088</td>\n",
              "      <td>-3.967915</td>\n",
              "      <td>-6.161645</td>\n",
              "      <td>-2.237080</td>\n",
              "      <td>-2.162160</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.3</td>\n",
              "      <td>-0.203043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>280 rows Ã— 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0af27ded-39b0-4a03-86a8-6894f34cf3d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0af27ded-39b0-4a03-86a8-6894f34cf3d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0af27ded-39b0-4a03-86a8-6894f34cf3d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lags(data, tickers, covs, period):\n",
        "    lagged_covs = []  # List to store the names of lagged columns\n",
        "\n",
        "    for covariate in covs:\n",
        "        for i in period:\n",
        "            lagged_column_name = f'{covariate}_lag{i}'\n",
        "            data[lagged_column_name] = data[covariate].shift(i)\n",
        "            lagged_covs.append(lagged_column_name)\n",
        "\n",
        "    lagged_tickers = []  # List to store the names of lagged columns\n",
        "\n",
        "    for tick in tickers:\n",
        "        for i in period:\n",
        "            lagged_column_name = f'{tick}_lag{i}'\n",
        "            data[lagged_column_name] = data[tick].shift(i)\n",
        "            lagged_tickers.append(lagged_column_name)\n",
        "\n",
        "    data.dropna(inplace=True)\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    return data, lagged_covs, lagged_tickers"
      ],
      "metadata": {
        "id": "txJqK7hWK7UJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = ['AZN', 'BMY', 'JNJ', 'LLY', 'MRK', 'NVO', 'NVS', 'PFE','ROG']\n",
        "covs = ['unemployment_rate', 'CPI', 'treasury_yield', 'GDP_growth', 'SP500_return', 'inflation_change', 'unemp_change', 'treasury_yield_change']\n",
        "period = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "\n",
        "data, lagged_covs, lagged_tickers = create_lags(df, tickers, covs, period)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTZORTL_LFHU",
        "outputId": "b14faeb8-5865-4af5-f903-1aa1688b95cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n",
            "<ipython-input-6-fe5811118561>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  data[lagged_column_name] = data[tick].shift(i)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "gKE0KC_-LL3V",
        "outputId": "163586d2-42ea-4919-fb2a-6d554610e041"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Date  unemployment_rate      CPI  treasury_yield  GDP_growth  \\\n",
              "12   2001-02-01                4.2  176.000        5.160952   -0.195020   \n",
              "13   2001-03-01                4.3  176.100        5.098947   -0.217255   \n",
              "14   2001-04-01                4.4  176.400        4.885455    0.756989   \n",
              "15   2001-05-01                4.3  177.300        5.141000    0.461042   \n",
              "16   2001-06-01                4.5  177.700        5.391364   -0.691442   \n",
              "..          ...                ...      ...             ...         ...   \n",
              "275  2023-01-01                3.4  300.536        3.616190    0.390254   \n",
              "276  2023-02-01                3.6  301.648        3.531500   -0.442183   \n",
              "277  2023-03-01                3.5  301.808        3.746842   -0.442183   \n",
              "278  2023-04-01                3.4  302.918        3.663043   -0.442183   \n",
              "279  2023-05-01                3.7  304.127        3.460000   -0.442183   \n",
              "\n",
              "     SP500_return       AZN       BMY        JNJ        LLY  ...   ROG_lag3  \\\n",
              "12      -9.538746  4.143180  2.851534   4.509829   0.837527  ...   1.748252   \n",
              "13      -5.857225  5.615486 -6.323922  -9.828843  -3.186580  ...  12.886598   \n",
              "14       8.836250 -1.512969 -5.723911  10.300706  10.879156  ...  -5.022831   \n",
              "15      -0.560577  0.631283 -2.660517   0.487142  -0.352891  ...  -8.846156   \n",
              "16      -2.659796 -2.237527 -3.576694   3.433253 -12.344135  ...  -0.112520   \n",
              "..            ...       ...       ...        ...        ...  ...        ...   \n",
              "275      6.776820 -3.584079  0.972908  -7.489384  -5.928822  ...  -2.707956   \n",
              "276     -2.514271 -0.290649 -4.328217  -6.217115  -9.568502  ... -53.665066   \n",
              "277      3.313488  8.035329  0.507544   1.862736  10.703390  ...   9.446071   \n",
              "278      1.985238  5.489119 -3.664707   5.612908  15.269915  ...  16.968326   \n",
              "279      0.461619 -0.191204 -2.695194  -5.277949   8.487855  ...   5.451681   \n",
              "\n",
              "      ROG_lag4   ROG_lag5   ROG_lag6   ROG_lag7   ROG_lag8   ROG_lag9  \\\n",
              "12   13.043478  -9.318996  -2.447552   2.142857  13.131313  -8.163265   \n",
              "13    1.748252  13.043478  -9.318996  -2.447552   2.142857  13.131313   \n",
              "14   12.886598   1.748252  13.043478  -9.318996  -2.447552   2.142857   \n",
              "15   -5.022831  12.886598   1.748252  13.043478  -9.318996  -2.447552   \n",
              "16   -8.846156  -5.022831  12.886598   1.748252  13.043478  -9.318996   \n",
              "..         ...        ...        ...        ...        ...        ...   \n",
              "275  -3.448826  -6.956359   2.731887  -1.239735  -1.972516  -0.360696   \n",
              "276  -2.707956  -3.448826  -6.956359   2.731887  -1.239735  -1.972516   \n",
              "277 -53.665066  -2.707956  -3.448826  -6.956359   2.731887  -1.239735   \n",
              "278   9.446071 -53.665066  -2.707956  -3.448826  -6.956359   2.731887   \n",
              "279  16.968326   9.446071 -53.665066  -2.707956  -3.448826  -6.956359   \n",
              "\n",
              "     ROG_lag10  ROG_lag11  ROG_lag12  \n",
              "12    7.370518   6.922258  54.440789  \n",
              "13   -8.163265   7.370518   6.922258  \n",
              "14   13.131313  -8.163265   7.370518  \n",
              "15    2.142857  13.131313  -8.163265  \n",
              "16   -2.447552   2.142857  13.131313  \n",
              "..         ...        ...        ...  \n",
              "275  -0.476186   0.018314  -0.018311  \n",
              "276  -0.360696  -0.476186   0.018314  \n",
              "277  -1.972516  -0.360696  -0.476186  \n",
              "278  -1.239735  -1.972516  -0.360696  \n",
              "279   2.731887  -1.239735  -1.972516  \n",
              "\n",
              "[268 rows x 222 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39bbeb1f-ea7b-4d33-ab72-8a912fd383db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>unemployment_rate</th>\n",
              "      <th>CPI</th>\n",
              "      <th>treasury_yield</th>\n",
              "      <th>GDP_growth</th>\n",
              "      <th>SP500_return</th>\n",
              "      <th>AZN</th>\n",
              "      <th>BMY</th>\n",
              "      <th>JNJ</th>\n",
              "      <th>LLY</th>\n",
              "      <th>...</th>\n",
              "      <th>ROG_lag3</th>\n",
              "      <th>ROG_lag4</th>\n",
              "      <th>ROG_lag5</th>\n",
              "      <th>ROG_lag6</th>\n",
              "      <th>ROG_lag7</th>\n",
              "      <th>ROG_lag8</th>\n",
              "      <th>ROG_lag9</th>\n",
              "      <th>ROG_lag10</th>\n",
              "      <th>ROG_lag11</th>\n",
              "      <th>ROG_lag12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2001-02-01</td>\n",
              "      <td>4.2</td>\n",
              "      <td>176.000</td>\n",
              "      <td>5.160952</td>\n",
              "      <td>-0.195020</td>\n",
              "      <td>-9.538746</td>\n",
              "      <td>4.143180</td>\n",
              "      <td>2.851534</td>\n",
              "      <td>4.509829</td>\n",
              "      <td>0.837527</td>\n",
              "      <td>...</td>\n",
              "      <td>1.748252</td>\n",
              "      <td>13.043478</td>\n",
              "      <td>-9.318996</td>\n",
              "      <td>-2.447552</td>\n",
              "      <td>2.142857</td>\n",
              "      <td>13.131313</td>\n",
              "      <td>-8.163265</td>\n",
              "      <td>7.370518</td>\n",
              "      <td>6.922258</td>\n",
              "      <td>54.440789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2001-03-01</td>\n",
              "      <td>4.3</td>\n",
              "      <td>176.100</td>\n",
              "      <td>5.098947</td>\n",
              "      <td>-0.217255</td>\n",
              "      <td>-5.857225</td>\n",
              "      <td>5.615486</td>\n",
              "      <td>-6.323922</td>\n",
              "      <td>-9.828843</td>\n",
              "      <td>-3.186580</td>\n",
              "      <td>...</td>\n",
              "      <td>12.886598</td>\n",
              "      <td>1.748252</td>\n",
              "      <td>13.043478</td>\n",
              "      <td>-9.318996</td>\n",
              "      <td>-2.447552</td>\n",
              "      <td>2.142857</td>\n",
              "      <td>13.131313</td>\n",
              "      <td>-8.163265</td>\n",
              "      <td>7.370518</td>\n",
              "      <td>6.922258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2001-04-01</td>\n",
              "      <td>4.4</td>\n",
              "      <td>176.400</td>\n",
              "      <td>4.885455</td>\n",
              "      <td>0.756989</td>\n",
              "      <td>8.836250</td>\n",
              "      <td>-1.512969</td>\n",
              "      <td>-5.723911</td>\n",
              "      <td>10.300706</td>\n",
              "      <td>10.879156</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.022831</td>\n",
              "      <td>12.886598</td>\n",
              "      <td>1.748252</td>\n",
              "      <td>13.043478</td>\n",
              "      <td>-9.318996</td>\n",
              "      <td>-2.447552</td>\n",
              "      <td>2.142857</td>\n",
              "      <td>13.131313</td>\n",
              "      <td>-8.163265</td>\n",
              "      <td>7.370518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2001-05-01</td>\n",
              "      <td>4.3</td>\n",
              "      <td>177.300</td>\n",
              "      <td>5.141000</td>\n",
              "      <td>0.461042</td>\n",
              "      <td>-0.560577</td>\n",
              "      <td>0.631283</td>\n",
              "      <td>-2.660517</td>\n",
              "      <td>0.487142</td>\n",
              "      <td>-0.352891</td>\n",
              "      <td>...</td>\n",
              "      <td>-8.846156</td>\n",
              "      <td>-5.022831</td>\n",
              "      <td>12.886598</td>\n",
              "      <td>1.748252</td>\n",
              "      <td>13.043478</td>\n",
              "      <td>-9.318996</td>\n",
              "      <td>-2.447552</td>\n",
              "      <td>2.142857</td>\n",
              "      <td>13.131313</td>\n",
              "      <td>-8.163265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2001-06-01</td>\n",
              "      <td>4.5</td>\n",
              "      <td>177.700</td>\n",
              "      <td>5.391364</td>\n",
              "      <td>-0.691442</td>\n",
              "      <td>-2.659796</td>\n",
              "      <td>-2.237527</td>\n",
              "      <td>-3.576694</td>\n",
              "      <td>3.433253</td>\n",
              "      <td>-12.344135</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.112520</td>\n",
              "      <td>-8.846156</td>\n",
              "      <td>-5.022831</td>\n",
              "      <td>12.886598</td>\n",
              "      <td>1.748252</td>\n",
              "      <td>13.043478</td>\n",
              "      <td>-9.318996</td>\n",
              "      <td>-2.447552</td>\n",
              "      <td>2.142857</td>\n",
              "      <td>13.131313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>2023-01-01</td>\n",
              "      <td>3.4</td>\n",
              "      <td>300.536</td>\n",
              "      <td>3.616190</td>\n",
              "      <td>0.390254</td>\n",
              "      <td>6.776820</td>\n",
              "      <td>-3.584079</td>\n",
              "      <td>0.972908</td>\n",
              "      <td>-7.489384</td>\n",
              "      <td>-5.928822</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.707956</td>\n",
              "      <td>-3.448826</td>\n",
              "      <td>-6.956359</td>\n",
              "      <td>2.731887</td>\n",
              "      <td>-1.239735</td>\n",
              "      <td>-1.972516</td>\n",
              "      <td>-0.360696</td>\n",
              "      <td>-0.476186</td>\n",
              "      <td>0.018314</td>\n",
              "      <td>-0.018311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>2023-02-01</td>\n",
              "      <td>3.6</td>\n",
              "      <td>301.648</td>\n",
              "      <td>3.531500</td>\n",
              "      <td>-0.442183</td>\n",
              "      <td>-2.514271</td>\n",
              "      <td>-0.290649</td>\n",
              "      <td>-4.328217</td>\n",
              "      <td>-6.217115</td>\n",
              "      <td>-9.568502</td>\n",
              "      <td>...</td>\n",
              "      <td>-53.665066</td>\n",
              "      <td>-2.707956</td>\n",
              "      <td>-3.448826</td>\n",
              "      <td>-6.956359</td>\n",
              "      <td>2.731887</td>\n",
              "      <td>-1.239735</td>\n",
              "      <td>-1.972516</td>\n",
              "      <td>-0.360696</td>\n",
              "      <td>-0.476186</td>\n",
              "      <td>0.018314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>2023-03-01</td>\n",
              "      <td>3.5</td>\n",
              "      <td>301.808</td>\n",
              "      <td>3.746842</td>\n",
              "      <td>-0.442183</td>\n",
              "      <td>3.313488</td>\n",
              "      <td>8.035329</td>\n",
              "      <td>0.507544</td>\n",
              "      <td>1.862736</td>\n",
              "      <td>10.703390</td>\n",
              "      <td>...</td>\n",
              "      <td>9.446071</td>\n",
              "      <td>-53.665066</td>\n",
              "      <td>-2.707956</td>\n",
              "      <td>-3.448826</td>\n",
              "      <td>-6.956359</td>\n",
              "      <td>2.731887</td>\n",
              "      <td>-1.239735</td>\n",
              "      <td>-1.972516</td>\n",
              "      <td>-0.360696</td>\n",
              "      <td>-0.476186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>2023-04-01</td>\n",
              "      <td>3.4</td>\n",
              "      <td>302.918</td>\n",
              "      <td>3.663043</td>\n",
              "      <td>-0.442183</td>\n",
              "      <td>1.985238</td>\n",
              "      <td>5.489119</td>\n",
              "      <td>-3.664707</td>\n",
              "      <td>5.612908</td>\n",
              "      <td>15.269915</td>\n",
              "      <td>...</td>\n",
              "      <td>16.968326</td>\n",
              "      <td>9.446071</td>\n",
              "      <td>-53.665066</td>\n",
              "      <td>-2.707956</td>\n",
              "      <td>-3.448826</td>\n",
              "      <td>-6.956359</td>\n",
              "      <td>2.731887</td>\n",
              "      <td>-1.239735</td>\n",
              "      <td>-1.972516</td>\n",
              "      <td>-0.360696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>2023-05-01</td>\n",
              "      <td>3.7</td>\n",
              "      <td>304.127</td>\n",
              "      <td>3.460000</td>\n",
              "      <td>-0.442183</td>\n",
              "      <td>0.461619</td>\n",
              "      <td>-0.191204</td>\n",
              "      <td>-2.695194</td>\n",
              "      <td>-5.277949</td>\n",
              "      <td>8.487855</td>\n",
              "      <td>...</td>\n",
              "      <td>5.451681</td>\n",
              "      <td>16.968326</td>\n",
              "      <td>9.446071</td>\n",
              "      <td>-53.665066</td>\n",
              "      <td>-2.707956</td>\n",
              "      <td>-3.448826</td>\n",
              "      <td>-6.956359</td>\n",
              "      <td>2.731887</td>\n",
              "      <td>-1.239735</td>\n",
              "      <td>-1.972516</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>268 rows Ã— 222 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39bbeb1f-ea7b-4d33-ab72-8a912fd383db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39bbeb1f-ea7b-4d33-ab72-8a912fd383db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39bbeb1f-ea7b-4d33-ab72-8a912fd383db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 month ahead"
      ],
      "metadata": {
        "id": "Rs69uPRAgbG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18/06/2023"
      ],
      "metadata": {
        "id": "dB4LMrvbci14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import arch\n",
        "\n",
        "def one_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha, t):\n",
        "    VaR_forecasts = {}\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_tickers_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag')]\n",
        "\n",
        "        # Prepare the data\n",
        "        train_data = data.loc[:t]\n",
        "        test_data = data.loc[t:]\n",
        "\n",
        "        # Specify the regressors and dependent variable\n",
        "        X_train = train_data[lagged_covs + lagged_tickers_ticker]\n",
        "        y_train = train_data[ticker]\n",
        "\n",
        "        # Fit the GARCH(1, 1) model\n",
        "        garch_model = arch.arch_model(y_train, x=X_train, vol='Garch', p=1, q=1)\n",
        "        garch_model_result = garch_model.fit()\n",
        "\n",
        "        # Obtain the predicted conditional volatility for t+h\n",
        "        X_test = test_data[lagged_covs + lagged_tickers_ticker]\n",
        "        conditional_volatility = garch_model_result.conditional_volatility\n",
        "        VaR_forecast = conditional_volatility * np.percentile(garch_model_result.resid / conditional_volatility, alpha * 100)\n",
        "\n",
        "        # Compute loss\n",
        "        r = test_data[ticker]\n",
        "        loss = tick_loss(alpha, r, VaR_forecast)\n",
        "\n",
        "        VaR_forecasts[ticker] = VaR_forecast\n",
        "        losses[ticker] = loss\n",
        "\n",
        "    return VaR_forecasts, losses\n"
      ],
      "metadata": {
        "id": "FTAyoxzILTAj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.05\n",
        "t = 255\n",
        "VaR_forecasts, losses = one_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha, t)\n",
        "\n",
        "# Access VaR forecasts and losses\n",
        "for ticker in tickers:\n",
        "    print(f'Ticker: {ticker}')\n",
        "    print('VaR forecast:', VaR_forecasts[ticker])\n",
        "    print('Loss:', losses[ticker])\n",
        "    print('---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzqc9Nu6LWP4",
        "outputId": "328a8702-9a63-47d5-a502-750150e1e943"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1159.7636996500732\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 849.6317953982542\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 937.2452486616035\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 840.3751966360928\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 840.3670078951413\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 840.3446163506405\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 840.3430701695563\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 840.3430238214773\n",
            "Iteration:      9,   Func. Count:     52,   Neg. LLF: 840.3430238214983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 840.3430238214773\n",
            "            Iterations: 9\n",
            "            Function evaluations: 52\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 876.7003857081925\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 889.3557645435415\n",
            "Iteration:      3,   Func. Count:     19,   Neg. LLF: 872.726750089291\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 857.598346742981\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 857.5943953026263\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 857.5927134533298\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 857.581925754989\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 857.551845666982\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 857.5110447626643\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 857.4737981769581\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 857.4316530780507\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 857.4196825218146\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 857.4191418511459\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 857.419105966716\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 857.4191040831072\n",
            "Iteration:     16,   Func. Count:     85,   Neg. LLF: 857.4191040831308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 857.4191040831072\n",
            "            Iterations: 16\n",
            "            Function evaluations: 85\n",
            "            Gradient evaluations: 16\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 136348755.0961463\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 1128.4906287458045\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 748.6390826665566\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 749.0835703792047\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 747.4560577252767\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 742.0518291214466\n",
            "Iteration:      7,   Func. Count:     45,   Neg. LLF: 741.845826607861\n",
            "Iteration:      8,   Func. Count:     51,   Neg. LLF: 741.8373952071781\n",
            "Iteration:      9,   Func. Count:     57,   Neg. LLF: 741.8365406630396\n",
            "Iteration:     10,   Func. Count:     62,   Neg. LLF: 741.836540077783\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 741.836540077783\n",
            "            Iterations: 10\n",
            "            Function evaluations: 62\n",
            "            Gradient evaluations: 10\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1212.5634737557841\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 897.079383289743\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 853.3850129207542\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 833.5812277154168\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 833.540824940505\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 833.5236201942864\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 833.5213529893995\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 833.5212835725034\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 833.5212826052682\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 833.5212826052682\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 970.8821845361399\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 11769.889291743795\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 852.1121816370958\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 842.1713619359076\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 841.8299414570446\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 841.8199907902888\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 841.8199045567492\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 841.8199040606873\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 841.8199040606873\n",
            "            Iterations: 8\n",
            "            Function evaluations: 49\n",
            "            Gradient evaluations: 8\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1932.3735579222598\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1197.0781359411403\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 857.6135671198253\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 851.4068874978846\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 851.398134712121\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 851.3546243312614\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 851.2924664505352\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 851.2780141089689\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 851.2757031171037\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 851.2749176515492\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 851.2748987022435\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 851.2748886874972\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 851.2748880716983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 851.2748880716983\n",
            "            Iterations: 13\n",
            "            Function evaluations: 71\n",
            "            Gradient evaluations: 13\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1077.6099789123105\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1213.622379783466\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 872.062964806496\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 770.4995249034941\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 770.4522576366847\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 770.4436016261117\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 770.4431022197208\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 770.4430590287492\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 770.443058525653\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 770.443058525653\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1038.1194476046944\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 826.6747012172495\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 1067.7587851808598\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 812.0828012904778\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 811.9997105039965\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 813.3829188680027\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 811.9764948566119\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 811.9674156140086\n",
            "Iteration:      9,   Func. Count:     54,   Neg. LLF: 811.9667545266398\n",
            "Iteration:     10,   Func. Count:     59,   Neg. LLF: 811.9667290699651\n",
            "Iteration:     11,   Func. Count:     64,   Neg. LLF: 811.9667224947991\n",
            "Iteration:     12,   Func. Count:     69,   Neg. LLF: 811.9667219616308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 811.9667219616308\n",
            "            Iterations: 12\n",
            "            Function evaluations: 69\n",
            "            Gradient evaluations: 12\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1209.1403758137285\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 986.7696941746776\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 984.8933363378974\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 984.8920062555766\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 984.8919876942392\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 984.8919726042732\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 984.8919010711488\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 984.8917390018631\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 984.8913204943328\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 984.8902787142376\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 984.8878495712363\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 984.8834254476269\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 984.8782829269476\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 984.8762848574709\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 984.875906761326\n",
            "Iteration:     16,   Func. Count:     86,   Neg. LLF: 984.8758842422201\n",
            "Iteration:     17,   Func. Count:     90,   Neg. LLF: 984.8758842423346\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 984.8758842422201\n",
            "            Iterations: 17\n",
            "            Function evaluations: 90\n",
            "            Gradient evaluations: 17\n",
            "Ticker: AZN\n",
            "VaR forecast: 0      -9.996021\n",
            "1      -9.601825\n",
            "2      -9.469887\n",
            "3      -9.212307\n",
            "4      -8.966501\n",
            "         ...    \n",
            "251   -10.264120\n",
            "252    -9.687177\n",
            "253    -9.429598\n",
            "254   -10.179981\n",
            "255    -9.626088\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n",
            "Ticker: BMY\n",
            "VaR forecast: 0     -13.669566\n",
            "1     -11.854618\n",
            "2     -12.523722\n",
            "3     -12.469089\n",
            "4     -11.806255\n",
            "         ...    \n",
            "251   -16.033292\n",
            "252   -12.493708\n",
            "253   -12.280615\n",
            "254   -12.146785\n",
            "255   -11.666081\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n",
            "Ticker: JNJ\n",
            "VaR forecast: 0     -9.006160\n",
            "1     -8.837722\n",
            "2     -9.074430\n",
            "3     -9.195315\n",
            "4     -8.961772\n",
            "         ...   \n",
            "251   -7.649429\n",
            "252   -7.517181\n",
            "253   -7.526152\n",
            "254   -7.667123\n",
            "255   -7.538154\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n",
            "Ticker: LLY\n",
            "VaR forecast: 0     -11.001460\n",
            "1     -10.344141\n",
            "2      -9.995758\n",
            "3     -10.547572\n",
            "4      -9.974363\n",
            "         ...    \n",
            "251   -12.384337\n",
            "252   -13.005040\n",
            "253   -12.110167\n",
            "254   -13.132357\n",
            "255   -12.224829\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n",
            "Ticker: MRK\n",
            "VaR forecast: 0     -12.706572\n",
            "1     -12.477762\n",
            "2     -12.393246\n",
            "3     -12.127547\n",
            "4     -11.984090\n",
            "         ...    \n",
            "251   -11.916453\n",
            "252   -11.885537\n",
            "253   -11.876275\n",
            "254   -11.835917\n",
            "255   -11.938498\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n",
            "Ticker: NVO\n",
            "VaR forecast: 0     -13.565808\n",
            "1     -12.518081\n",
            "2     -11.677133\n",
            "3     -10.990794\n",
            "4     -10.458990\n",
            "         ...    \n",
            "251   -10.519521\n",
            "252   -11.549828\n",
            "253   -10.893856\n",
            "254   -10.710848\n",
            "255   -10.257819\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n",
            "Ticker: NVS\n",
            "VaR forecast: 0     -7.412679\n",
            "1     -7.211786\n",
            "2     -7.864858\n",
            "3     -7.599597\n",
            "4     -7.449779\n",
            "         ...   \n",
            "251   -8.969655\n",
            "252   -8.613198\n",
            "253   -8.271752\n",
            "254   -7.966662\n",
            "255   -7.842334\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n",
            "Ticker: PFE\n",
            "VaR forecast: 0      -9.311767\n",
            "1      -8.933974\n",
            "2      -9.830658\n",
            "3      -9.741771\n",
            "4      -9.338294\n",
            "         ...    \n",
            "251   -15.197717\n",
            "252   -15.373276\n",
            "253   -15.421421\n",
            "254   -15.259578\n",
            "255   -14.596792\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n",
            "Ticker: ROG\n",
            "VaR forecast: 0     -16.883722\n",
            "1     -17.534358\n",
            "2     -17.188975\n",
            "3     -19.737439\n",
            "4     -18.858608\n",
            "         ...    \n",
            "251   -20.552320\n",
            "252   -18.766749\n",
            "253   -17.817983\n",
            "254   -17.340379\n",
            "255   -17.096751\n",
            "Name: cond_vol, Length: 256, dtype: float64\n",
            "Loss: nan\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19/06/2023"
      ],
      "metadata": {
        "id": "H96G0dSxclNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tick_loss(alpha, returns, var):\n",
        "    df = pd.DataFrame({'Return': returns, 'VaR': var})\n",
        "    df['Indicator'] = np.where(df['Return'] < df['VaR'], 1, 0)\n",
        "\n",
        "    t_loss = 0\n",
        "\n",
        "    for i in df.index:\n",
        "        t_loss += (\n",
        "            alpha * (df['Return'][i] - df['VaR'][i]) * (1 - df['Indicator'][i])\n",
        "            + (1 - alpha) * (df['VaR'][i] - df['Return'][i]) * df['Indicator'][i]\n",
        "        )\n",
        "\n",
        "    return t_loss, df"
      ],
      "metadata": {
        "id": "P7-TtqRtcp_U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IN SAMPLE"
      ],
      "metadata": {
        "id": "sRLMZglgI658"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def in_sample_forecast(data, lagged_covs, lagged_tickers, tickers, alpha, t_start, t_end):\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_tickers_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag')]\n",
        "\n",
        "        # Prepare the data\n",
        "        X = data.loc[t_start:t_end, lagged_covs + lagged_tickers_ticker]\n",
        "        y = data.loc[t_start:t_end, ticker]\n",
        "\n",
        "        garch_model = arch.arch_model(y, vol='Garch', p=1, q=1)\n",
        "        garch_results = garch_model.fit()\n",
        "\n",
        "        # Perform in-sample forecast\n",
        "        conditional_volatility = garch_results.conditional_volatility\n",
        "\n",
        "        standardized_residuals = garch_results.resid / conditional_volatility\n",
        "        z_value = np.percentile(standardized_residuals, alpha * 100)\n",
        "        predicted_var = conditional_volatility * z_value\n",
        "\n",
        "        loss, df = tick_loss(alpha, y, predicted_var)\n",
        "\n",
        "        losses[ticker] = (loss, df)\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "sKTSl_InI6hM"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_start = 255  # Start index of the test set\n",
        "t_end = 279  # End index of the test set\n",
        "\n",
        "losses_one_step_ahead_insample = in_sample_forecast(data, lagged_covs, lagged_tickers, tickers, alpha=0.05, t_start=t_start, t_end=t_end)\n",
        "\n",
        "# Print losses for the test set observations\n",
        "for ticker, loss_tuple in losses_one_step_ahead_insample.items():\n",
        "    print(f\"Ticker: {ticker}\")\n",
        "    loss, df = loss_tuple\n",
        "    print(f\"Tick Loss: {loss}\")\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPuYp_CLJK6c",
        "outputId": "7382a9c7-8280-4658-eef6-b7fe37cc82b3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 46.67421103301048\n",
            "Iteration:      2,   Func. Count:     12,   Neg. LLF: 44.36956374278404\n",
            "Iteration:      3,   Func. Count:     17,   Neg. LLF: 44.36932770269599\n",
            "Iteration:      4,   Func. Count:     22,   Neg. LLF: 44.36893010880799\n",
            "Iteration:      5,   Func. Count:     27,   Neg. LLF: 44.3688678173408\n",
            "Iteration:      6,   Func. Count:     32,   Neg. LLF: 44.368688106275066\n",
            "Iteration:      7,   Func. Count:     37,   Neg. LLF: 44.36831786910732\n",
            "Iteration:      8,   Func. Count:     42,   Neg. LLF: 44.36723144960223\n",
            "Iteration:      9,   Func. Count:     47,   Neg. LLF: 44.36436343283679\n",
            "Iteration:     10,   Func. Count:     52,   Neg. LLF: 44.355779713391634\n",
            "Iteration:     11,   Func. Count:     57,   Neg. LLF: 44.342581700974094\n",
            "Iteration:     12,   Func. Count:     62,   Neg. LLF: 44.29956136943187\n",
            "Iteration:     13,   Func. Count:     67,   Neg. LLF: 44.25567983227127\n",
            "Iteration:     14,   Func. Count:     72,   Neg. LLF: 44.22344208936078\n",
            "Iteration:     15,   Func. Count:     77,   Neg. LLF: 44.222193172601386\n",
            "Iteration:     16,   Func. Count:     82,   Neg. LLF: 44.222189642284306\n",
            "Iteration:     17,   Func. Count:     86,   Neg. LLF: 44.22218965338259\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 44.222189642284306\n",
            "            Iterations: 17\n",
            "            Function evaluations: 86\n",
            "            Gradient evaluations: 17\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 43.49458855369818\n",
            "Iteration:      2,   Func. Count:     12,   Neg. LLF: 42.84690801683896\n",
            "Iteration:      3,   Func. Count:     18,   Neg. LLF: 42.74011614348738\n",
            "Iteration:      4,   Func. Count:     24,   Neg. LLF: 42.735427612757285\n",
            "Iteration:      5,   Func. Count:     29,   Neg. LLF: 42.73502126050103\n",
            "Iteration:      6,   Func. Count:     34,   Neg. LLF: 42.734996864178804\n",
            "Iteration:      7,   Func. Count:     39,   Neg. LLF: 42.73498443705544\n",
            "Iteration:      8,   Func. Count:     44,   Neg. LLF: 42.73497909249034\n",
            "Iteration:      9,   Func. Count:     49,   Neg. LLF: 42.73495348422453\n",
            "Iteration:     10,   Func. Count:     54,   Neg. LLF: 42.7349030595368\n",
            "Iteration:     11,   Func. Count:     59,   Neg. LLF: 42.7347789751492\n",
            "Iteration:     12,   Func. Count:     64,   Neg. LLF: 42.73456559284716\n",
            "Iteration:     13,   Func. Count:     69,   Neg. LLF: 42.73431660883504\n",
            "Iteration:     14,   Func. Count:     74,   Neg. LLF: 42.73417736894465\n",
            "Iteration:     15,   Func. Count:     79,   Neg. LLF: 42.73415076119463\n",
            "Iteration:     16,   Func. Count:     84,   Neg. LLF: 42.734149628154135\n",
            "Iteration:     17,   Func. Count:     88,   Neg. LLF: 42.73414962815411\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 42.734149628154135\n",
            "            Iterations: 17\n",
            "            Function evaluations: 88\n",
            "            Gradient evaluations: 17\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 39.614096923304366\n",
            "Iteration:      2,   Func. Count:     11,   Neg. LLF: 39.2989330011418\n",
            "Iteration:      3,   Func. Count:     16,   Neg. LLF: 39.94353340520804\n",
            "Iteration:      4,   Func. Count:     22,   Neg. LLF: 39.48809191493664\n",
            "Iteration:      5,   Func. Count:     28,   Neg. LLF: 39.21567037674468\n",
            "Iteration:      6,   Func. Count:     33,   Neg. LLF: 39.21441398487523\n",
            "Iteration:      7,   Func. Count:     38,   Neg. LLF: 39.21158364057078\n",
            "Iteration:      8,   Func. Count:     43,   Neg. LLF: 39.19840166890411\n",
            "Iteration:      9,   Func. Count:     48,   Neg. LLF: 39.17744499658629\n",
            "Iteration:     10,   Func. Count:     53,   Neg. LLF: 39.1302242191899\n",
            "Iteration:     11,   Func. Count:     58,   Neg. LLF: 39.11152984442847\n",
            "Iteration:     12,   Func. Count:     63,   Neg. LLF: 39.105339690946046\n",
            "Iteration:     13,   Func. Count:     68,   Neg. LLF: 39.10521750002408\n",
            "Iteration:     14,   Func. Count:     73,   Neg. LLF: 39.10519608762491\n",
            "Iteration:     15,   Func. Count:     78,   Neg. LLF: 39.10519512848056\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 39.10519512848056\n",
            "            Iterations: 15\n",
            "            Function evaluations: 78\n",
            "            Gradient evaluations: 15\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 40.192687120188495\n",
            "Iteration:      2,   Func. Count:     12,   Neg. LLF: 37.82377216262537\n",
            "Iteration:      3,   Func. Count:     17,   Neg. LLF: 37.823624367697114\n",
            "Iteration:      4,   Func. Count:     22,   Neg. LLF: 37.823134315864046\n",
            "Iteration:      5,   Func. Count:     27,   Neg. LLF: 37.82179761169812\n",
            "Iteration:      6,   Func. Count:     32,   Neg. LLF: 37.818711812177135\n",
            "Iteration:      7,   Func. Count:     37,   Neg. LLF: 37.81134004271231\n",
            "Iteration:      8,   Func. Count:     42,   Neg. LLF: 37.79521755191631\n",
            "Iteration:      9,   Func. Count:     47,   Neg. LLF: 37.77638852802876\n",
            "Iteration:     10,   Func. Count:     52,   Neg. LLF: 37.77407211873169\n",
            "Iteration:     11,   Func. Count:     57,   Neg. LLF: 37.77372711887454\n",
            "Iteration:     12,   Func. Count:     62,   Neg. LLF: 37.773725087659685\n",
            "Iteration:     13,   Func. Count:     66,   Neg. LLF: 37.77372508938055\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 37.773725087659685\n",
            "            Iterations: 13\n",
            "            Function evaluations: 66\n",
            "            Gradient evaluations: 13\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 47.03749262767302\n",
            "Iteration:      2,   Func. Count:     12,   Neg. LLF: 44.730666001517775\n",
            "Iteration:      3,   Func. Count:     17,   Neg. LLF: 44.73056916989733\n",
            "Iteration:      4,   Func. Count:     22,   Neg. LLF: 44.730535205531694\n",
            "Iteration:      5,   Func. Count:     27,   Neg. LLF: 44.730367514614656\n",
            "Iteration:      6,   Func. Count:     32,   Neg. LLF: 44.72974030216668\n",
            "Iteration:      7,   Func. Count:     37,   Neg. LLF: 44.72902006278467\n",
            "Iteration:      8,   Func. Count:     42,   Neg. LLF: 44.72822389592948\n",
            "Iteration:      9,   Func. Count:     47,   Neg. LLF: 44.72490490684421\n",
            "Iteration:     10,   Func. Count:     52,   Neg. LLF: 44.71708618186796\n",
            "Iteration:     11,   Func. Count:     57,   Neg. LLF: 44.70059186266244\n",
            "Iteration:     12,   Func. Count:     62,   Neg. LLF: 44.68376411632363\n",
            "Iteration:     13,   Func. Count:     67,   Neg. LLF: 44.643104465324846\n",
            "Iteration:     14,   Func. Count:     72,   Neg. LLF: 44.63831282545695\n",
            "Iteration:     15,   Func. Count:     77,   Neg. LLF: 44.63824880052193\n",
            "Iteration:     16,   Func. Count:     81,   Neg. LLF: 44.6382488057212\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 44.63824880052193\n",
            "            Iterations: 16\n",
            "            Function evaluations: 81\n",
            "            Gradient evaluations: 16\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 46.43295377065247\n",
            "Iteration:      2,   Func. Count:     12,   Neg. LLF: 42.293161106361396\n",
            "Iteration:      3,   Func. Count:     17,   Neg. LLF: 42.293142105702366\n",
            "Iteration:      4,   Func. Count:     22,   Neg. LLF: 42.293136308336535\n",
            "Iteration:      5,   Func. Count:     27,   Neg. LLF: 42.29312487119886\n",
            "Iteration:      6,   Func. Count:     32,   Neg. LLF: 42.29311694189655\n",
            "Iteration:      7,   Func. Count:     37,   Neg. LLF: 42.29309413340833\n",
            "Iteration:      8,   Func. Count:     42,   Neg. LLF: 42.2930438808238\n",
            "Iteration:      9,   Func. Count:     47,   Neg. LLF: 42.29290681751049\n",
            "Iteration:     10,   Func. Count:     52,   Neg. LLF: 42.292556669949\n",
            "Iteration:     11,   Func. Count:     57,   Neg. LLF: 42.291669199716225\n",
            "Iteration:     12,   Func. Count:     62,   Neg. LLF: 42.28975484820203\n",
            "Iteration:     13,   Func. Count:     67,   Neg. LLF: 42.287727507475275\n",
            "Iteration:     14,   Func. Count:     72,   Neg. LLF: 42.28583539533311\n",
            "Iteration:     15,   Func. Count:     77,   Neg. LLF: 42.28499618695965\n",
            "Iteration:     16,   Func. Count:     82,   Neg. LLF: 42.284993306929714\n",
            "Iteration:     17,   Func. Count:     87,   Neg. LLF: 42.28499255282846\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 42.28499255282846\n",
            "            Iterations: 17\n",
            "            Function evaluations: 87\n",
            "            Gradient evaluations: 17\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 44.87679756862421\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 43.46088821984601\n",
            "Iteration:      3,   Func. Count:     18,   Neg. LLF: 43.46088547517515\n",
            "Iteration:      4,   Func. Count:     23,   Neg. LLF: 43.46088233604787\n",
            "Iteration:      5,   Func. Count:     27,   Neg. LLF: 43.46088233605088\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 43.46088233604787\n",
            "            Iterations: 5\n",
            "            Function evaluations: 27\n",
            "            Gradient evaluations: 5\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 43.422023184182784\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 42.62553393254653\n",
            "Iteration:      3,   Func. Count:     18,   Neg. LLF: 42.625529773020276\n",
            "Iteration:      4,   Func. Count:     23,   Neg. LLF: 42.62552722510277\n",
            "Iteration:      5,   Func. Count:     27,   Neg. LLF: 42.62552722513798\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 42.62552722510277\n",
            "            Iterations: 5\n",
            "            Function evaluations: 27\n",
            "            Gradient evaluations: 5\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 58.61193972992924\n",
            "Iteration:      2,   Func. Count:     12,   Neg. LLF: 54.65169888142495\n",
            "Iteration:      3,   Func. Count:     16,   Neg. LLF: 54.651698881462075\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 54.65169888142495\n",
            "            Iterations: 3\n",
            "            Function evaluations: 16\n",
            "            Gradient evaluations: 3\n",
            "Ticker: NVS\n",
            "Tick Loss: 6.19205146976383\n",
            "        Return       VaR  Indicator\n",
            "255   3.305696 -7.499351          0\n",
            "256  -7.048606 -7.630489          0\n",
            "257   1.537918 -7.759410          0\n",
            "258  -6.186653 -7.886225          0\n",
            "259  -5.601082 -8.011032          0\n",
            "260   6.735944 -8.133924          0\n",
            "261  10.390734 -8.254987          0\n",
            "262   1.295221 -8.374300          0\n",
            "263  -0.110227 -8.491936          0\n",
            "264  -7.172811 -8.607965          0\n",
            "265   9.367574 -8.722451          0\n",
            "266  16.334413 -8.835454          0\n",
            "267  -6.161645 -8.947029          0\n",
            "Ticker: AZN\n",
            "Tick Loss: 8.669023933390456\n",
            "        Return        VaR  Indicator\n",
            "255   0.120491 -10.520149          0\n",
            "256  -0.616736  -8.528607          0\n",
            "257   0.242173  -8.186344          0\n",
            "258  -5.813079  -7.992087          0\n",
            "259 -11.466275 -10.342817          1\n",
            "260   7.239243 -14.783584          0\n",
            "261  15.575584 -11.368913          0\n",
            "262  -0.250105 -16.544688          0\n",
            "263  -3.584079 -10.289562          0\n",
            "264  -0.290649  -9.549774          0\n",
            "265   8.035329  -8.351904          0\n",
            "266   5.489119 -10.467624          0\n",
            "267  -0.191204  -9.495464          0\n",
            "Ticker: BMY\n",
            "Tick Loss: 8.294791055518102\n",
            "        Return        VaR  Indicator\n",
            "255   0.239147  -8.227181          0\n",
            "256   2.054343  -5.932715          0\n",
            "257  -3.516240  -6.800243          0\n",
            "258  -8.633766  -5.423692          1\n",
            "259   5.459121  -9.225979          0\n",
            "260   8.974528 -10.274986          0\n",
            "261   4.416367 -14.000474          0\n",
            "262 -10.376181 -10.019060          1\n",
            "263   0.972908 -11.416175          0\n",
            "264  -4.328217  -6.956380          0\n",
            "265   0.507544  -5.840099          0\n",
            "266  -3.664707  -5.704183          0\n",
            "267  -2.695194  -5.321095          0\n",
            "Ticker: JNJ\n",
            "Tick Loss: 5.557341292215228\n",
            "       Return       VaR  Indicator\n",
            "255 -0.515348 -6.157590          0\n",
            "256 -0.489803 -6.247059          0\n",
            "257 -1.684398 -6.335265          0\n",
            "258 -7.552151 -6.422260          1\n",
            "259  1.932340 -6.508092          0\n",
            "260  6.494844 -6.592806          0\n",
            "261  2.316498 -6.676446          0\n",
            "262 -0.117873 -6.759051          0\n",
            "263 -7.489384 -6.840658          1\n",
            "264 -6.217115 -6.921303          0\n",
            "265  1.862736 -7.001020          0\n",
            "266  5.612908 -7.079839          0\n",
            "267 -5.277949 -7.157790          0\n",
            "Ticker: LLY\n",
            "Tick Loss: 10.25804068091057\n",
            "        Return        VaR  Indicator\n",
            "255   7.294686 -11.338438          0\n",
            "256   3.791026 -11.522885          0\n",
            "257   1.683984 -11.704426          0\n",
            "258  -8.632348 -11.883194          0\n",
            "259   7.693656 -12.059312          0\n",
            "260  11.980826 -12.232895          0\n",
            "261   2.482801 -12.404048          0\n",
            "262  -1.137072 -12.572873          0\n",
            "263  -5.928822 -12.739460          0\n",
            "264  -9.568502 -12.903896          0\n",
            "265  10.703390 -13.066264          0\n",
            "266  15.269915 -13.226638          0\n",
            "267   8.487855 -13.385091          0\n",
            "Ticker: MRK\n",
            "Tick Loss: 5.593016842211913\n",
            "        Return       VaR  Indicator\n",
            "255   3.765922 -6.306819          0\n",
            "256  -0.934478 -6.376426          0\n",
            "257  -1.205267 -6.426627          0\n",
            "258  -4.454882 -6.462929          0\n",
            "259   0.890348 -6.489231          0\n",
            "260  18.457777 -6.508311          0\n",
            "261   8.814239 -6.522168          0\n",
            "262   0.753719 -6.532237          0\n",
            "263  -2.549213 -6.539558          0\n",
            "264  -1.089288 -6.544882          0\n",
            "265   0.141189 -6.548756          0\n",
            "266   9.289214 -6.551575          0\n",
            "267  -4.382088 -6.553626          0\n",
            "Ticker: NVO\n",
            "Tick Loss: 8.30164498835955\n",
            "        Return       VaR  Indicator\n",
            "255  -3.157891 -9.720596          0\n",
            "256   0.932963 -9.751219          0\n",
            "257   4.155071 -9.766903          0\n",
            "258  -8.581764 -9.774946          0\n",
            "259  -5.591229 -9.779072          0\n",
            "260   9.244201 -9.781190          0\n",
            "261  14.479974 -9.782278          0\n",
            "262   8.619586 -9.782836          0\n",
            "263   2.541749 -9.783122          0\n",
            "264   1.592445 -9.783269          0\n",
            "265  12.873250 -9.783345          0\n",
            "266   5.836894 -9.783384          0\n",
            "267  -3.967915 -9.783404          0\n",
            "Ticker: PFE\n",
            "Tick Loss: 9.41392182193261\n",
            "        Return        VaR  Indicator\n",
            "255   8.090479 -10.025491          0\n",
            "256  -0.343961  -9.948621          0\n",
            "257  -3.662024  -9.911083          0\n",
            "258  -9.758538  -9.892806          0\n",
            "259  -3.250061  -9.883920          0\n",
            "260   6.375682  -9.879602          0\n",
            "261   7.690670  -9.877506          0\n",
            "262   3.090302  -9.876487          0\n",
            "263 -13.817335  -9.875993          1\n",
            "264  -7.286115  -9.875753          0\n",
            "265   0.566924  -9.875636          0\n",
            "266  -4.681371  -9.875580          0\n",
            "267  -2.237080  -9.875552          0\n",
            "Ticker: ROG\n",
            "Tick Loss: 43.96027555760463\n",
            "        Return        VaR  Indicator\n",
            "255  -1.972516 -22.630766          0\n",
            "256  -1.239735 -23.036236          0\n",
            "257   2.731887 -23.253979          0\n",
            "258  -6.956359 -23.371652          0\n",
            "259  -3.448826 -23.435458          0\n",
            "260  -2.707956 -23.470117          0\n",
            "261 -53.665066 -23.488962          1\n",
            "262   9.446071 -23.499213          0\n",
            "263  16.968326 -23.504792          0\n",
            "264   5.451681 -23.507828          0\n",
            "265  11.025813 -23.509480          0\n",
            "266  -1.517467 -23.510380          0\n",
            "267  -2.162160 -23.510870          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OUT OF SAMPLE"
      ],
      "metadata": {
        "id": "l4ImLRIvI4KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arch\n",
        "\n",
        "def one_step_ahead_VaR(data, lagged_covs, lagged_tickers, tickers, alpha, t):\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_tickers_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag')]\n",
        "\n",
        "        # Prepare the data\n",
        "        train_data = data.loc[:t]\n",
        "        test_data = data.loc[t:]\n",
        "\n",
        "        X_train = train_data[lagged_covs + lagged_tickers_ticker]\n",
        "        y_train = train_data[ticker]\n",
        "\n",
        "        garch_model = arch.arch_model(y_train, vol='Garch', p=1, q=1)\n",
        "        garch_results = garch_model.fit()\n",
        "\n",
        "        X_test = test_data[lagged_covs + lagged_tickers_ticker]\n",
        "        y_test = test_data[ticker]\n",
        "\n",
        "        # Perform rolling forecast\n",
        "        forecasts = garch_results.forecast(start=t, horizon=len(y_test))\n",
        "        cond_vol_forecast = forecasts.variance[-1:].apply(lambda x: np.sqrt(x))\n",
        "        mean_forecast = forecasts.mean[-1:]\n",
        "\n",
        "        epsilon = np.random.normal(0, 1, size=(len(y_test), 10000))\n",
        "        VaR_forecast = mean_forecast.values[0] + cond_vol_forecast.values[0] * np.percentile(epsilon, alpha * 100, axis=1)\n",
        "\n",
        "        loss, df = tick_loss(alpha, y_test, VaR_forecast.flatten())\n",
        "\n",
        "        losses[ticker] = (loss, df)\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "B1-8Wm2gfshZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_one_step_ahead = one_step_ahead_VaR(data, lagged_covs, lagged_tickers, tickers, alpha=0.05,t=255)\n",
        "\n",
        "# Print losses\n",
        "for ticker, loss_tuple in losses_one_step_ahead.items():\n",
        "    print(f\"Ticker: {ticker}\")\n",
        "    loss, df = loss_tuple\n",
        "    print(f\"Tick Loss: {loss}\")\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjJ3QWk3fwEi",
        "outputId": "cf76297e-729b-4e6a-fe2c-e573bf2a6b8f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1159.7636996500732\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 849.6317953982542\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 937.2452486616035\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 840.3751966360928\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 840.3670078951413\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 840.3446163506405\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 840.3430701695563\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 840.3430238214773\n",
            "Iteration:      9,   Func. Count:     52,   Neg. LLF: 840.3430238214983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 840.3430238214773\n",
            "            Iterations: 9\n",
            "            Function evaluations: 52\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 876.7003857081925\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 889.3557645435415\n",
            "Iteration:      3,   Func. Count:     19,   Neg. LLF: 872.726750089291\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 857.598346742981\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 857.5943953026263\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 857.5927134533298\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 857.581925754989\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 857.551845666982\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 857.5110447626643\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 857.4737981769581\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 857.4316530780507\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 857.4196825218146\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 857.4191418511459\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 857.419105966716\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 857.4191040831072\n",
            "Iteration:     16,   Func. Count:     85,   Neg. LLF: 857.4191040831308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 857.4191040831072\n",
            "            Iterations: 16\n",
            "            Function evaluations: 85\n",
            "            Gradient evaluations: 16\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 136348755.0961463\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 1128.4906287458045\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 748.6390826665566\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 749.0835703792047\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 747.4560577252767\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 742.0518291214466\n",
            "Iteration:      7,   Func. Count:     45,   Neg. LLF: 741.845826607861\n",
            "Iteration:      8,   Func. Count:     51,   Neg. LLF: 741.8373952071781\n",
            "Iteration:      9,   Func. Count:     57,   Neg. LLF: 741.8365406630396\n",
            "Iteration:     10,   Func. Count:     62,   Neg. LLF: 741.836540077783\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 741.836540077783\n",
            "            Iterations: 10\n",
            "            Function evaluations: 62\n",
            "            Gradient evaluations: 10\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1212.5634737557841\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 897.079383289743\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 853.3850129207542\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 833.5812277154168\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 833.540824940505\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 833.5236201942864\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 833.5213529893995\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 833.5212835725034\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 833.5212826052682\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 833.5212826052682\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 970.8821845361399\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 11769.889291743795\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 852.1121816370958\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 842.1713619359076\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 841.8299414570446\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 841.8199907902888\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 841.8199045567492\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 841.8199040606873\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 841.8199040606873\n",
            "            Iterations: 8\n",
            "            Function evaluations: 49\n",
            "            Gradient evaluations: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1932.3735579222598\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1197.0781359411403\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 857.6135671198253\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 851.4068874978846\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 851.398134712121\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 851.3546243312614\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 851.2924664505352\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 851.2780141089689\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 851.2757031171037\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 851.2749176515492\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 851.2748987022435\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 851.2748886874972\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 851.2748880716983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 851.2748880716983\n",
            "            Iterations: 13\n",
            "            Function evaluations: 71\n",
            "            Gradient evaluations: 13\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1077.6099789123105\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1213.622379783466\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 872.062964806496\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 770.4995249034941\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 770.4522576366847\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 770.4436016261117\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 770.4431022197208\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 770.4430590287492\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 770.443058525653\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 770.443058525653\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1038.1194476046944\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 826.6747012172495\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 1067.7587851808598\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 812.0828012904778\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 811.9997105039965\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 813.3829188680027\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 811.9764948566119\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 811.9674156140086\n",
            "Iteration:      9,   Func. Count:     54,   Neg. LLF: 811.9667545266398\n",
            "Iteration:     10,   Func. Count:     59,   Neg. LLF: 811.9667290699651\n",
            "Iteration:     11,   Func. Count:     64,   Neg. LLF: 811.9667224947991\n",
            "Iteration:     12,   Func. Count:     69,   Neg. LLF: 811.9667219616308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 811.9667219616308\n",
            "            Iterations: 12\n",
            "            Function evaluations: 69\n",
            "            Gradient evaluations: 12\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1209.1403758137285\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 986.7696941746776\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 984.8933363378974\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 984.8920062555766\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 984.8919876942392\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 984.8919726042732\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 984.8919010711488\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 984.8917390018631\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 984.8913204943328\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 984.8902787142376\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 984.8878495712363\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 984.8834254476269\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 984.8782829269476\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 984.8762848574709\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 984.875906761326\n",
            "Iteration:     16,   Func. Count:     86,   Neg. LLF: 984.8758842422201\n",
            "Iteration:     17,   Func. Count:     90,   Neg. LLF: 984.8758842423346\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 984.8758842422201\n",
            "            Iterations: 17\n",
            "            Function evaluations: 90\n",
            "            Gradient evaluations: 17\n",
            "Ticker: AZN\n",
            "Tick Loss: 9.138581763055099\n",
            "        Return       VaR  Indicator\n",
            "255   0.120491 -9.100613          0\n",
            "256  -0.616736 -8.943612          0\n",
            "257   0.242173 -9.476305          0\n",
            "258  -5.813079 -9.404355          0\n",
            "259 -11.466275 -9.250248          1\n",
            "260   7.239243 -9.593210          0\n",
            "261  15.575584 -9.505227          0\n",
            "262  -0.250105 -9.820482          0\n",
            "263  -3.584079 -9.805305          0\n",
            "264  -0.290649 -9.770586          0\n",
            "265   8.035329 -9.710178          0\n",
            "266   5.489119 -9.817388          0\n",
            "267  -0.191204 -9.763786          0\n",
            "Ticker: BMY\n",
            "Tick Loss: 6.343333544615302\n",
            "        Return        VaR  Indicator\n",
            "255   0.239147  -9.262441          0\n",
            "256   2.054343 -10.468040          0\n",
            "257  -3.516240 -10.728334          0\n",
            "258  -8.633766 -10.648104          0\n",
            "259   5.459121 -10.755344          0\n",
            "260   8.974528 -10.797412          0\n",
            "261   4.416367 -10.911850          0\n",
            "262 -10.376181 -10.711773          0\n",
            "263   0.972908 -10.475696          0\n",
            "264  -4.328217 -10.898274          0\n",
            "265   0.507544 -10.577957          0\n",
            "266  -3.664707 -10.562762          0\n",
            "267  -2.695194 -10.659030          0\n",
            "Ticker: JNJ\n",
            "Tick Loss: 5.770651818944219\n",
            "       Return       VaR  Indicator\n",
            "255 -0.515348 -6.441561          0\n",
            "256 -0.489803 -6.475132          0\n",
            "257 -1.684398 -6.435356          0\n",
            "258 -7.552151 -6.456893          1\n",
            "259  1.932340 -6.532361          0\n",
            "260  6.494844 -6.697711          0\n",
            "261  2.316498 -6.380964          0\n",
            "262 -0.117873 -6.416767          0\n",
            "263 -7.489384 -6.469106          1\n",
            "264 -6.217115 -6.367946          0\n",
            "265  1.862736 -6.496218          0\n",
            "266  5.612908 -6.573878          0\n",
            "267 -5.277949 -6.483122          0\n",
            "Ticker: LLY\n",
            "Tick Loss: 9.298103486195258\n",
            "        Return        VaR  Indicator\n",
            "255   7.294686 -11.502832          0\n",
            "256   3.791026 -11.754202          0\n",
            "257   1.683984 -11.416185          0\n",
            "258  -8.632348 -11.012141          0\n",
            "259   7.693656 -10.975889          0\n",
            "260  11.980826 -10.925674          0\n",
            "261   2.482801 -10.685126          0\n",
            "262  -1.137072 -10.806909          0\n",
            "263  -5.928822 -10.702409          0\n",
            "264  -9.568502 -10.466381          0\n",
            "265  10.703390 -10.409460          0\n",
            "266  15.269915 -10.579344          0\n",
            "267   8.487855 -10.604124          0\n",
            "Ticker: MRK\n",
            "Tick Loss: 8.003976507747296\n",
            "        Return        VaR  Indicator\n",
            "255   3.765922 -10.132510          0\n",
            "256  -0.934478 -10.236523          0\n",
            "257  -1.205267 -10.299637          0\n",
            "258  -4.454882 -10.235331          0\n",
            "259   0.890348 -10.312457          0\n",
            "260  18.457777 -10.399601          0\n",
            "261   8.814239 -10.075856          0\n",
            "262   0.753719 -10.182008          0\n",
            "263  -2.549213 -10.170605          0\n",
            "264  -1.089288 -10.045960          0\n",
            "265   0.141189 -10.258353          0\n",
            "266   9.289214 -10.177058          0\n",
            "267  -4.382088 -10.056438          0\n",
            "Ticker: NVO\n",
            "Tick Loss: 7.921892796630264\n",
            "        Return       VaR  Indicator\n",
            "255  -3.157891 -8.550288          0\n",
            "256   0.932963 -9.035394          0\n",
            "257   4.155071 -9.200583          0\n",
            "258  -8.581764 -9.234591          0\n",
            "259  -5.591229 -9.296515          0\n",
            "260   9.244201 -9.211233          0\n",
            "261  14.479974 -9.243165          0\n",
            "262   8.619586 -9.090279          0\n",
            "263   2.541749 -9.361853          0\n",
            "264   1.592445 -9.409004          0\n",
            "265  12.873250 -9.129806          0\n",
            "266   5.836894 -9.445702          0\n",
            "267  -3.967915 -9.252108          0\n",
            "Ticker: NVS\n",
            "Tick Loss: 5.530633544762222\n",
            "        Return       VaR  Indicator\n",
            "255   3.305696 -7.063495          0\n",
            "256  -7.048606 -7.333580          0\n",
            "257   1.537918 -7.042263          0\n",
            "258  -6.186653 -7.135114          0\n",
            "259  -5.601082 -7.296174          0\n",
            "260   6.735944 -7.127330          0\n",
            "261  10.390734 -7.262987          0\n",
            "262   1.295221 -7.116089          0\n",
            "263  -0.110227 -7.392939          0\n",
            "264  -7.172811 -7.285506          0\n",
            "265   9.367574 -7.208508          0\n",
            "266  16.334413 -7.352400          0\n",
            "267  -6.161645 -7.309809          0\n",
            "Ticker: PFE\n",
            "Tick Loss: 8.676139558612595\n",
            "        Return        VaR  Indicator\n",
            "255   8.090479 -13.278246          0\n",
            "256  -0.343961 -13.401571          0\n",
            "257  -3.662024 -13.178743          0\n",
            "258  -9.758538 -12.579556          0\n",
            "259  -3.250061 -12.593048          0\n",
            "260   6.375682 -12.704912          0\n",
            "261   7.690670 -12.662636          0\n",
            "262   3.090302 -12.074404          0\n",
            "263 -13.817335 -12.289466          1\n",
            "264  -7.286115 -12.156444          0\n",
            "265   0.566924 -11.895311          0\n",
            "266  -4.681371 -11.821255          0\n",
            "267  -2.237080 -11.552257          0\n",
            "Ticker: ROG\n",
            "Tick Loss: 46.18574863214244\n",
            "        Return        VaR  Indicator\n",
            "255  -1.972516 -15.748862          0\n",
            "256  -1.239735 -16.805209          0\n",
            "257   2.731887 -16.787783          0\n",
            "258  -6.956359 -16.735235          0\n",
            "259  -3.448826 -16.975952          0\n",
            "260  -2.707956 -17.614212          0\n",
            "261 -53.665066 -17.186923          1\n",
            "262   9.446071 -17.638189          0\n",
            "263  16.968326 -17.651181          0\n",
            "264   5.451681 -17.404240          0\n",
            "265  11.025813 -16.937408          0\n",
            "266  -1.517467 -17.322861          0\n",
            "267  -2.162160 -17.390363          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 months ahead"
      ],
      "metadata": {
        "id": "3acT0MxDgVbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IN SAMPLE"
      ],
      "metadata": {
        "id": "V5UmANDJM9H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import arch\n",
        "\n",
        "def in_sample_forecast(data, lagged_covs, lagged_tickers, tickers, alpha):\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_tickers_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag')]\n",
        "\n",
        "        # Prepare the data\n",
        "        X = data[lagged_covs + lagged_tickers_ticker]\n",
        "        y = data[ticker]\n",
        "\n",
        "        garch_model = arch.arch_model(y, vol='Garch', p=1, q=1)\n",
        "        garch_results = garch_model.fit()\n",
        "\n",
        "        # Perform in-sample forecast\n",
        "        forecast_horizon = 3  # 3-month forecast horizon\n",
        "        num_observations = len(y) - forecast_horizon\n",
        "\n",
        "        for i in range(num_observations):\n",
        "            start_index = i\n",
        "            end_index = i + forecast_horizon\n",
        "            X_test = X.iloc[start_index:end_index]\n",
        "            y_test = y.iloc[start_index:end_index]\n",
        "\n",
        "            forecasts = garch_results.forecast(start=start_index, horizon=forecast_horizon)\n",
        "            cond_vol_forecast = forecasts.variance.iloc[-1].apply(lambda x: np.sqrt(x))\n",
        "            mean_forecast = forecasts.mean.iloc[-1]\n",
        "\n",
        "            epsilon = np.random.normal(0, 1, size=(forecast_horizon, 10000))\n",
        "            VaR_forecast = mean_forecast + cond_vol_forecast * np.percentile(epsilon, alpha * 100, axis=1)\n",
        "\n",
        "            loss, _ = tick_loss(alpha, y_test, VaR_forecast.flatten())\n",
        "            losses[(ticker, start_index)] = loss\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "uMa8L1jiNZ1p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.05\n",
        "\n",
        "losses_insample_3months = in_sample_forecast(data, lagged_covs, lagged_tickers, tickers, alpha)\n",
        "\n",
        "# Print losses for the in-sample forecast\n",
        "for (ticker, start_index), loss in losses_insample_3months.items():\n",
        "    print(f\"Ticker: {ticker}, Start Index: {start_index}\")\n",
        "    print(f\"Tick Loss: {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "GdtfjuTkNbwh",
        "outputId": "e4fbd9ea-51a1-4024-ea8e-fb2c95dfdf24"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1220.2413727683897\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 891.4787040618978\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 965.078161133711\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 880.4715932694294\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 880.469766678912\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 880.4677341105024\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 880.4658609203732\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 880.4653909287848\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 880.4653460425961\n",
            "Iteration:     10,   Func. Count:     58,   Neg. LLF: 880.4653451269519\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 880.4653451269519\n",
            "            Iterations: 10\n",
            "            Function evaluations: 58\n",
            "            Gradient evaluations: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-46495a5561cc>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlosses_insample_3months\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_sample_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagged_covs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagged_tickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print losses for the in-sample forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-db5d00e162d5>\u001b[0m in \u001b[0;36min_sample_forecast\u001b[0;34m(data, lagged_covs, lagged_tickers, tickers, alpha)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mVaR_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_forecast\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcond_vol_forecast\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtick_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVaR_forecast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'flatten'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OUT OF SAMPLE"
      ],
      "metadata": {
        "id": "A5UBZQVbM3Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tick_loss(alpha, returns, var):\n",
        "    df = pd.DataFrame({'Return': returns, 'VaR': var})\n",
        "    df['Indicator'] = np.where(df['Return'] < df['VaR'], 1, 0)\n",
        "\n",
        "    t_loss = 0\n",
        "\n",
        "    for i in df.index:\n",
        "        t_loss += (\n",
        "            alpha * (df['Return'][i] - df['VaR'][i]) * (1 - df['Indicator'][i])\n",
        "            + (1 - alpha) * (df['VaR'][i] - df['Return'][i]) * df['Indicator'][i]\n",
        "        )\n",
        "\n",
        "    return t_loss, df"
      ],
      "metadata": {
        "id": "avq3lVB1fwfg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arch\n",
        "import numpy as np\n",
        "\n",
        "def three_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha, t):\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_covs_t3 = [cov for cov in lagged_covs if int(cov.split('_lag')[1]) >= 3]\n",
        "        lagged_tickers_t3_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag') and int(tick.split('_lag')[1]) >= 3]\n",
        "\n",
        "        # Prepare the data\n",
        "        train_data = data.loc[:t]\n",
        "        test_data = data.loc[t:]\n",
        "\n",
        "        X_train = train_data[lagged_covs_t3 + lagged_tickers_t3_ticker]\n",
        "        y_train = train_data[ticker]\n",
        "\n",
        "        garch_model = arch.arch_model(y_train, vol='Garch', p=1, q=1)\n",
        "        garch_results = garch_model.fit()\n",
        "\n",
        "        X_test = test_data[lagged_covs_t3 + lagged_tickers_t3_ticker]\n",
        "        y_test = test_data[ticker]\n",
        "\n",
        "        # Perform rolling forecast\n",
        "        forecasts = garch_results.forecast(start=t, horizon=len(y_test))\n",
        "        cond_vol_forecast = forecasts.variance[-1:].apply(lambda x: np.sqrt(x))\n",
        "        mean_forecast = forecasts.mean[-1:]\n",
        "\n",
        "        epsilon = np.random.normal(0, 1, size=(len(y_test), 10000))\n",
        "        VaR_forecast = mean_forecast.values[0] + cond_vol_forecast.values[0] * np.percentile(epsilon, alpha * 100, axis=1)\n",
        "\n",
        "        loss, df = tick_loss(alpha, y_test, VaR_forecast.flatten())\n",
        "\n",
        "        losses[ticker] = (loss, df)\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "px0W0K0IwUlE"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_three_step_ahead = three_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha=0.05, t=255)\n",
        "\n",
        "# Print losses\n",
        "for ticker, loss_tuple in losses_three_step_ahead.items():\n",
        "    print(f\"Ticker: {ticker}\")\n",
        "    loss, df = loss_tuple\n",
        "    print(f\"Tick Loss: {loss}\")\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCUYTTnzw2Zy",
        "outputId": "40ae26b7-a5c1-44d3-a0ce-bc439f959ed2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1077.6099789123105\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1213.622379783466\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 872.062964806496\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 770.4995249034941\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 770.4522576366847\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 770.4436016261117\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 770.4431022197208\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 770.4430590287492\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 770.443058525653\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 770.443058525653\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1159.7636996500732\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 849.6317953982542\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 937.2452486616035\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 840.3751966360928\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 840.3670078951413\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 840.3446163506405\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 840.3430701695563\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 840.3430238214773\n",
            "Iteration:      9,   Func. Count:     52,   Neg. LLF: 840.3430238214983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 840.3430238214773\n",
            "            Iterations: 9\n",
            "            Function evaluations: 52\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 876.7003857081925\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 889.3557645435415\n",
            "Iteration:      3,   Func. Count:     19,   Neg. LLF: 872.726750089291\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 857.598346742981\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 857.5943953026263\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 857.5927134533298\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 857.581925754989\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 857.551845666982\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 857.5110447626643\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 857.4737981769581\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 857.4316530780507\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 857.4196825218146\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 857.4191418511459\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 857.419105966716\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 857.4191040831072\n",
            "Iteration:     16,   Func. Count:     85,   Neg. LLF: 857.4191040831308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 857.4191040831072\n",
            "            Iterations: 16\n",
            "            Function evaluations: 85\n",
            "            Gradient evaluations: 16\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 136348755.0961463\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 1128.4906287458045\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 748.6390826665566\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 749.0835703792047\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 747.4560577252767\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 742.0518291214466\n",
            "Iteration:      7,   Func. Count:     45,   Neg. LLF: 741.845826607861\n",
            "Iteration:      8,   Func. Count:     51,   Neg. LLF: 741.8373952071781\n",
            "Iteration:      9,   Func. Count:     57,   Neg. LLF: 741.8365406630396\n",
            "Iteration:     10,   Func. Count:     62,   Neg. LLF: 741.836540077783\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 741.836540077783\n",
            "            Iterations: 10\n",
            "            Function evaluations: 62\n",
            "            Gradient evaluations: 10\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1212.5634737557841\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 897.079383289743\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 853.3850129207542\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 833.5812277154168\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 833.540824940505\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 833.5236201942864\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 833.5213529893995\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 833.5212835725034\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 833.5212826052682\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 833.5212826052682\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 970.8821845361399\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 11769.889291743795\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 852.1121816370958\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 842.1713619359076\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 841.8299414570446\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 841.8199907902888\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 841.8199045567492\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 841.8199040606873\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 841.8199040606873\n",
            "            Iterations: 8\n",
            "            Function evaluations: 49\n",
            "            Gradient evaluations: 8\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1932.3735579222598\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1197.0781359411403\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 857.6135671198253\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 851.4068874978846\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 851.398134712121\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 851.3546243312614\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 851.2924664505352\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 851.2780141089689\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 851.2757031171037\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 851.2749176515492\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 851.2748987022435\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 851.2748886874972\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 851.2748880716983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 851.2748880716983\n",
            "            Iterations: 13\n",
            "            Function evaluations: 71\n",
            "            Gradient evaluations: 13\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1038.1194476046944\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 826.6747012172495\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 1067.7587851808598\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 812.0828012904778\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 811.9997105039965\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 813.3829188680027\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 811.9764948566119\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 811.9674156140086\n",
            "Iteration:      9,   Func. Count:     54,   Neg. LLF: 811.9667545266398\n",
            "Iteration:     10,   Func. Count:     59,   Neg. LLF: 811.9667290699651\n",
            "Iteration:     11,   Func. Count:     64,   Neg. LLF: 811.9667224947991\n",
            "Iteration:     12,   Func. Count:     69,   Neg. LLF: 811.9667219616308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 811.9667219616308\n",
            "            Iterations: 12\n",
            "            Function evaluations: 69\n",
            "            Gradient evaluations: 12\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1209.1403758137285\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 986.7696941746776\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 984.8933363378974\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 984.8920062555766\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 984.8919876942392\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 984.8919726042732\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 984.8919010711488\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 984.8917390018631\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 984.8913204943328\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 984.8902787142376\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 984.8878495712363\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 984.8834254476269\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 984.8782829269476\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 984.8762848574709\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 984.875906761326\n",
            "Iteration:     16,   Func. Count:     86,   Neg. LLF: 984.8758842422201\n",
            "Iteration:     17,   Func. Count:     90,   Neg. LLF: 984.8758842423346\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 984.8758842422201\n",
            "            Iterations: 17\n",
            "            Function evaluations: 90\n",
            "            Gradient evaluations: 17\n",
            "Ticker: NVS\n",
            "Tick Loss: 5.526182132836725\n",
            "        Return       VaR  Indicator\n",
            "255   3.305696 -7.083764          0\n",
            "256  -7.048606 -7.200374          0\n",
            "257   1.537918 -7.201026          0\n",
            "258  -6.186653 -7.277079          0\n",
            "259  -5.601082 -7.293024          0\n",
            "260   6.735944 -7.338157          0\n",
            "261  10.390734 -7.169726          0\n",
            "262   1.295221 -7.227202          0\n",
            "263  -0.110227 -7.158175          0\n",
            "264  -7.172811 -7.234585          0\n",
            "265   9.367574 -7.177891          0\n",
            "266  16.334413 -7.092489          0\n",
            "267  -6.161645 -7.383675          0\n",
            "Ticker: AZN\n",
            "Tick Loss: 8.766574513167143\n",
            "        Return       VaR  Indicator\n",
            "255   0.120491 -8.940474          0\n",
            "256  -0.616736 -9.105856          0\n",
            "257   0.242173 -9.235867          0\n",
            "258  -5.813079 -9.431998          0\n",
            "259 -11.466275 -9.631456          1\n",
            "260   7.239243 -9.718850          0\n",
            "261  15.575584 -9.587097          0\n",
            "262  -0.250105 -9.703197          0\n",
            "263  -3.584079 -9.934778          0\n",
            "264  -0.290649 -9.631220          0\n",
            "265   8.035329 -9.727489          0\n",
            "266   5.489119 -9.793756          0\n",
            "267  -0.191204 -9.703255          0\n",
            "Ticker: BMY\n",
            "Tick Loss: 6.304752278393963\n",
            "        Return        VaR  Indicator\n",
            "255   0.239147  -9.402381          0\n",
            "256   2.054343 -10.465814          0\n",
            "257  -3.516240 -10.690531          0\n",
            "258  -8.633766 -10.468496          0\n",
            "259   5.459121 -10.660418          0\n",
            "260   8.974528 -10.724753          0\n",
            "261   4.416367 -10.540117          0\n",
            "262 -10.376181 -10.775530          0\n",
            "263   0.972908 -10.578506          0\n",
            "264  -4.328217 -10.267780          0\n",
            "265   0.507544 -10.475192          0\n",
            "266  -3.664707 -10.784957          0\n",
            "267  -2.695194 -10.850916          0\n",
            "Ticker: JNJ\n",
            "Tick Loss: 5.79407331022755\n",
            "       Return       VaR  Indicator\n",
            "255 -0.515348 -6.608588          0\n",
            "256 -0.489803 -6.492425          0\n",
            "257 -1.684398 -6.448367          0\n",
            "258 -7.552151 -6.608584          1\n",
            "259  1.932340 -6.484640          0\n",
            "260  6.494844 -6.450829          0\n",
            "261  2.316498 -6.506611          0\n",
            "262 -0.117873 -6.429692          0\n",
            "263 -7.489384 -6.276739          1\n",
            "264 -6.217115 -6.504399          0\n",
            "265  1.862736 -6.466365          0\n",
            "266  5.612908 -6.320069          0\n",
            "267 -5.277949 -6.284621          0\n",
            "Ticker: LLY\n",
            "Tick Loss: 9.3352572455711\n",
            "        Return        VaR  Indicator\n",
            "255   7.294686 -11.940627          0\n",
            "256   3.791026 -11.541955          0\n",
            "257   1.683984 -11.428695          0\n",
            "258  -8.632348 -11.408075          0\n",
            "259   7.693656 -10.961303          0\n",
            "260  11.980826 -10.949414          0\n",
            "261   2.482801 -10.842263          0\n",
            "262  -1.137072 -10.665146          0\n",
            "263  -5.928822 -10.727601          0\n",
            "264  -9.568502 -10.554001          0\n",
            "265  10.703390 -10.723671          0\n",
            "266  15.269915 -10.374843          0\n",
            "267   8.487855 -10.466156          0\n",
            "Ticker: MRK\n",
            "Tick Loss: 7.951915794505399\n",
            "        Return        VaR  Indicator\n",
            "255   3.765922 -10.292336          0\n",
            "256  -0.934478 -10.101885          0\n",
            "257  -1.205267  -9.827480          0\n",
            "258  -4.454882 -10.074365          0\n",
            "259   0.890348 -10.414519          0\n",
            "260  18.457777  -9.944043          0\n",
            "261   8.814239 -10.242989          0\n",
            "262   0.753719 -10.196312          0\n",
            "263  -2.549213  -9.922440          0\n",
            "264  -1.089288 -10.193453          0\n",
            "265   0.141189 -10.111359          0\n",
            "266   9.289214 -10.235431          0\n",
            "267  -4.382088  -9.984512          0\n",
            "Ticker: NVO\n",
            "Tick Loss: 7.935196621028079\n",
            "        Return       VaR  Indicator\n",
            "255  -3.157891 -8.747197          0\n",
            "256   0.932963 -9.028093          0\n",
            "257   4.155071 -9.178758          0\n",
            "258  -8.581764 -9.021405          0\n",
            "259  -5.591229 -9.070577          0\n",
            "260   9.244201 -9.305628          0\n",
            "261  14.479974 -9.366476          0\n",
            "262   8.619586 -9.335826          0\n",
            "263   2.541749 -9.257641          0\n",
            "264   1.592445 -9.240264          0\n",
            "265  12.873250 -9.713353          0\n",
            "266   5.836894 -9.189255          0\n",
            "267  -3.967915 -9.272124          0\n",
            "Ticker: PFE\n",
            "Tick Loss: 8.737244988059098\n",
            "        Return        VaR  Indicator\n",
            "255   8.090479 -13.375160          0\n",
            "256  -0.343961 -13.467080          0\n",
            "257  -3.662024 -13.217462          0\n",
            "258  -9.758538 -12.987563          0\n",
            "259  -3.250061 -12.709210          0\n",
            "260   6.375682 -12.785489          0\n",
            "261   7.690670 -12.272823          0\n",
            "262   3.090302 -12.350626          0\n",
            "263 -13.817335 -12.277047          1\n",
            "264  -7.286115 -11.940394          0\n",
            "265   0.566924 -11.961871          0\n",
            "266  -4.681371 -11.758465          0\n",
            "267  -2.237080 -12.058389          0\n",
            "Ticker: ROG\n",
            "Tick Loss: 46.16217211178271\n",
            "        Return        VaR  Indicator\n",
            "255  -1.972516 -16.183157          0\n",
            "256  -1.239735 -16.698153          0\n",
            "257   2.731887 -17.040243          0\n",
            "258  -6.956359 -17.014765          0\n",
            "259  -3.448826 -17.227290          0\n",
            "260  -2.707956 -17.530639          0\n",
            "261 -53.665066 -17.285425          1\n",
            "262   9.446071 -17.279134          0\n",
            "263  16.968326 -17.432954          0\n",
            "264   5.451681 -17.791151          0\n",
            "265  11.025813 -17.356388          0\n",
            "266  -1.517467 -17.468998          0\n",
            "267  -2.162160 -17.388634          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rolling forecast for evaluation:"
      ],
      "metadata": {
        "id": "Pmw5aYli0WuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arch\n",
        "import pandas as pd\n",
        "\n",
        "def garch_rolling_forecast(data, ticker, alpha, dates):\n",
        "    t_loss = 0\n",
        "    dta = data.copy()\n",
        "\n",
        "    for d in dates:\n",
        "        split_date = d\n",
        "        train_df = dta[dta.Date <= split_date]\n",
        "        test_df = dta[dta.Date > split_date]\n",
        "        test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "        garch_model = arch.arch_model(train_df[ticker], vol='Garch', p=1, q=1)\n",
        "        garch_results = garch_model.fit()\n",
        "\n",
        "        forecasts = garch_results.forecast(start=0, horizon=3)\n",
        "        cond_vol_forecast = forecasts.variance[-1:].apply(lambda x: pd.Series([garch_results.conditional_volatility.iloc[-1] * np.sqrt(x)]*3))\n",
        "        mean_forecast = forecasts.mean[-1:]\n",
        "\n",
        "        epsilon = np.random.normal(0, 1, size=(3, 10000))\n",
        "        VaR_forecast = mean_forecast.values[0] + cond_vol_forecast.values[0] * np.percentile(epsilon, alpha * 100, axis=1)\n",
        "\n",
        "        r = test_df[ticker][0:3]\n",
        "        tickloss = tick_loss(alpha, r, VaR_forecast.flatten())\n",
        "\n",
        "        t_loss += tickloss\n",
        "\n",
        "    t_loss = t_loss / len(dates)\n",
        "    return t_loss\n"
      ],
      "metadata": {
        "id": "R25_4UoPzcaG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates3m = ['2022-05-01', '2022-06-01', '2022-07-01', '2022-08-01', '2022-09-01', '2022-10-01', '2022-11-01', '2022-12-01', '2023-01-01', '2023-02-01']"
      ],
      "metadata": {
        "id": "yo-i_d2pzdbw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = ['NVS', 'AZN', 'BMY', 'JNJ', 'LLY', 'MRK', 'NVO', 'PFE', 'ROG']\n",
        "\n",
        "for t in tickers:\n",
        "    loss = garch_rolling_forecast(data, t, 0.05, dates3m)\n",
        "    print(f'{t}: {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "6F7tVx2Rz-Io",
        "outputId": "3dd42729-799f-4421-cd8a-2cad8ac15909"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1077.6099789123105\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1213.622379783466\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 872.062964806496\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 770.4995249034941\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 770.4522576366847\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 770.4436016261117\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 770.4431022197208\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 770.4430590287492\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 770.443058525653\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 770.443058525653\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-0422966568bc>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgarch_rolling_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates3m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{t}: {loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-10d654df393a>\u001b[0m in \u001b[0;36mgarch_rolling_forecast\u001b[0;34m(data, ticker, alpha, dates)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtickloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtick_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVaR_forecast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mt_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtickloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-ef816597085b>\u001b[0m in \u001b[0;36mtick_loss\u001b[0;34m(alpha, returns, var)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtick_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Return'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VaR'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Indicator'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Return'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VaR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__lt__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__lt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__le__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6242\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6243\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cmp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_TEST_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 months ahead"
      ],
      "metadata": {
        "id": "CxLdgPZA6dAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tick_loss(alpha, returns, var):\n",
        "    df = pd.DataFrame({'Return': returns, 'VaR': var})\n",
        "    df['Indicator'] = np.where(df['Return'] < df['VaR'], 1, 0)\n",
        "\n",
        "    t_loss = 0\n",
        "\n",
        "    for i in df.index:\n",
        "        t_loss += (\n",
        "            alpha * (df['Return'][i] - df['VaR'][i]) * (1 - df['Indicator'][i])\n",
        "            + (1 - alpha) * (df['VaR'][i] - df['Return'][i]) * df['Indicator'][i]\n",
        "        )\n",
        "\n",
        "    return t_loss, df"
      ],
      "metadata": {
        "id": "GV31Ry550O8L"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arch\n",
        "import numpy as np\n",
        "\n",
        "def six_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha, t):\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_covs_t6 = [cov for cov in lagged_covs if int(cov.split('_lag')[1]) >= 6]\n",
        "        lagged_tickers_t6_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag') and int(tick.split('_lag')[1]) >= 6]\n",
        "\n",
        "        # Prepare the data\n",
        "        train_data = data.loc[:t]\n",
        "        test_data = data.loc[t:]\n",
        "\n",
        "        X_train = train_data[lagged_covs_t6 + lagged_tickers_t6_ticker]\n",
        "        y_train = train_data[ticker]\n",
        "\n",
        "        garch_model = arch.arch_model(y_train, vol='Garch', p=1, q=1)\n",
        "        garch_results = garch_model.fit()\n",
        "\n",
        "        X_test = test_data[lagged_covs_t6 + lagged_tickers_t6_ticker]\n",
        "        y_test = test_data[ticker]\n",
        "\n",
        "        # Perform rolling forecast\n",
        "        forecasts = garch_results.forecast(start=t, horizon=len(y_test) + 6)\n",
        "        cond_vol_forecast = forecasts.variance[-1:].apply(lambda x: np.sqrt(x))\n",
        "        mean_forecast = forecasts.mean[-1:]\n",
        "\n",
        "        epsilon = np.random.normal(0, 1, size=(len(y_test) + 6, 10000))\n",
        "        VaR_forecast = mean_forecast.values[0] + cond_vol_forecast.values[0] * np.percentile(epsilon, alpha * 100, axis=1)\n",
        "\n",
        "        loss, df = tick_loss(alpha, y_test, VaR_forecast[-len(y_test):].flatten())\n",
        "\n",
        "        losses[ticker] = (loss, df)\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "8JXyu34k6iRI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_six_step_ahead = six_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha=0.05, t=255)\n",
        "\n",
        "# Print losses\n",
        "for ticker, loss_tuple in losses_six_step_ahead.items():\n",
        "    print(f\"Ticker: {ticker}\")\n",
        "    loss, df = loss_tuple\n",
        "    print(f\"Tick Loss: {loss}\")\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlxUm9ez65La",
        "outputId": "cb6f2e72-3b11-4bff-c47d-11ba847becc6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1077.6099789123105\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1213.622379783466\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 872.062964806496\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 770.4995249034941\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 770.4522576366847\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 770.4436016261117\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 770.4431022197208\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 770.4430590287492\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 770.443058525653\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 770.443058525653\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1159.7636996500732\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 849.6317953982542\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 937.2452486616035\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 840.3751966360928\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 840.3670078951413\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 840.3446163506405\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 840.3430701695563\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 840.3430238214773\n",
            "Iteration:      9,   Func. Count:     52,   Neg. LLF: 840.3430238214983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 840.3430238214773\n",
            "            Iterations: 9\n",
            "            Function evaluations: 52\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 876.7003857081925\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 889.3557645435415\n",
            "Iteration:      3,   Func. Count:     19,   Neg. LLF: 872.726750089291\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 857.598346742981\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 857.5943953026263\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 857.5927134533298\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 857.581925754989\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 857.551845666982\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 857.5110447626643\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 857.4737981769581\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 857.4316530780507\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 857.4196825218146\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 857.4191418511459\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 857.419105966716\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 857.4191040831072\n",
            "Iteration:     16,   Func. Count:     85,   Neg. LLF: 857.4191040831308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 857.4191040831072\n",
            "            Iterations: 16\n",
            "            Function evaluations: 85\n",
            "            Gradient evaluations: 16\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 136348755.0961463\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 1128.4906287458045\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 748.6390826665566\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 749.0835703792047\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 747.4560577252767\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 742.0518291214466\n",
            "Iteration:      7,   Func. Count:     45,   Neg. LLF: 741.845826607861\n",
            "Iteration:      8,   Func. Count:     51,   Neg. LLF: 741.8373952071781\n",
            "Iteration:      9,   Func. Count:     57,   Neg. LLF: 741.8365406630396\n",
            "Iteration:     10,   Func. Count:     62,   Neg. LLF: 741.836540077783\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 741.836540077783\n",
            "            Iterations: 10\n",
            "            Function evaluations: 62\n",
            "            Gradient evaluations: 10\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1212.5634737557841\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 897.079383289743\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 853.3850129207542\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 833.5812277154168\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 833.540824940505\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 833.5236201942864\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 833.5213529893995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 833.5212835725034\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 833.5212826052682\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 833.5212826052682\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 970.8821845361399\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 11769.889291743795\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 852.1121816370958\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 842.1713619359076\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 841.8299414570446\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 841.8199907902888\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 841.8199045567492\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 841.8199040606873\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 841.8199040606873\n",
            "            Iterations: 8\n",
            "            Function evaluations: 49\n",
            "            Gradient evaluations: 8\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1932.3735579222598\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1197.0781359411403\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 857.6135671198253\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 851.4068874978846\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 851.398134712121\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 851.3546243312614\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 851.2924664505352\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 851.2780141089689\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 851.2757031171037\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 851.2749176515492\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 851.2748987022435\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 851.2748886874972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 851.2748880716983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 851.2748880716983\n",
            "            Iterations: 13\n",
            "            Function evaluations: 71\n",
            "            Gradient evaluations: 13\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1038.1194476046944\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 826.6747012172495\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 1067.7587851808598\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 812.0828012904778\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 811.9997105039965\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 813.3829188680027\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 811.9764948566119\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 811.9674156140086\n",
            "Iteration:      9,   Func. Count:     54,   Neg. LLF: 811.9667545266398\n",
            "Iteration:     10,   Func. Count:     59,   Neg. LLF: 811.9667290699651\n",
            "Iteration:     11,   Func. Count:     64,   Neg. LLF: 811.9667224947991\n",
            "Iteration:     12,   Func. Count:     69,   Neg. LLF: 811.9667219616308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 811.9667219616308\n",
            "            Iterations: 12\n",
            "            Function evaluations: 69\n",
            "            Gradient evaluations: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1209.1403758137285\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 986.7696941746776\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 984.8933363378974\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 984.8920062555766\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 984.8919876942392\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 984.8919726042732\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 984.8919010711488\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 984.8917390018631\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 984.8913204943328\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 984.8902787142376\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 984.8878495712363\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 984.8834254476269\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 984.8782829269476\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 984.8762848574709\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 984.875906761326\n",
            "Iteration:     16,   Func. Count:     86,   Neg. LLF: 984.8758842422201\n",
            "Iteration:     17,   Func. Count:     90,   Neg. LLF: 984.8758842423346\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 984.8758842422201\n",
            "            Iterations: 17\n",
            "            Function evaluations: 90\n",
            "            Gradient evaluations: 17\n",
            "Ticker: NVS\n",
            "Tick Loss: 5.587047641736124\n",
            "        Return       VaR  Indicator\n",
            "255   3.305696 -7.156837          0\n",
            "256  -7.048606 -7.163436          0\n",
            "257   1.537918 -7.234945          0\n",
            "258  -6.186653 -7.215172          0\n",
            "259  -5.601082 -7.196715          0\n",
            "260   6.735944 -7.311847          0\n",
            "261  10.390734 -7.404493          0\n",
            "262   1.295221 -7.513297          0\n",
            "263  -0.110227 -7.307130          0\n",
            "264  -7.172811 -7.156399          1\n",
            "265   9.367574 -7.334724          0\n",
            "266  16.334413 -7.410539          0\n",
            "267  -6.161645 -7.320704          0\n",
            "Ticker: AZN\n",
            "Tick Loss: 8.846663763999707\n",
            "        Return       VaR  Indicator\n",
            "255   0.120491 -9.778741          0\n",
            "256  -0.616736 -9.833778          0\n",
            "257   0.242173 -9.713696          0\n",
            "258  -5.813079 -9.663749          0\n",
            "259 -11.466275 -9.680050          1\n",
            "260   7.239243 -9.666864          0\n",
            "261  15.575584 -9.736992          0\n",
            "262  -0.250105 -9.769063          0\n",
            "263  -3.584079 -9.641276          0\n",
            "264  -0.290649 -9.775778          0\n",
            "265   8.035329 -9.745550          0\n",
            "266   5.489119 -9.844981          0\n",
            "267  -0.191204 -9.868446          0\n",
            "Ticker: BMY\n",
            "Tick Loss: 6.346647853417572\n",
            "        Return        VaR  Indicator\n",
            "255   0.239147 -10.528477          0\n",
            "256   2.054343 -10.420811          0\n",
            "257  -3.516240 -10.741114          0\n",
            "258  -8.633766 -10.669776          0\n",
            "259   5.459121 -10.582949          0\n",
            "260   8.974528 -10.803542          0\n",
            "261   4.416367 -10.856882          0\n",
            "262 -10.376181 -10.462282          0\n",
            "263   0.972908 -10.408543          0\n",
            "264  -4.328217 -10.148495          0\n",
            "265   0.507544 -10.690323          0\n",
            "266  -3.664707 -10.556564          0\n",
            "267  -2.695194 -10.653544          0\n",
            "Ticker: JNJ\n",
            "Tick Loss: 5.756514025495235\n",
            "       Return       VaR  Indicator\n",
            "255 -0.515348 -6.419825          0\n",
            "256 -0.489803 -6.509373          0\n",
            "257 -1.684398 -6.461321          0\n",
            "258 -7.552151 -6.413213          1\n",
            "259  1.932340 -6.428216          0\n",
            "260  6.494844 -6.392719          0\n",
            "261  2.316498 -6.363940          0\n",
            "262 -0.117873 -6.430120          0\n",
            "263 -7.489384 -6.482343          1\n",
            "264 -6.217115 -6.378829          0\n",
            "265  1.862736 -6.393375          0\n",
            "266  5.612908 -6.317537          0\n",
            "267 -5.277949 -6.344593          0\n",
            "Ticker: LLY\n",
            "Tick Loss: 8.984278115442125\n",
            "        Return        VaR  Indicator\n",
            "255   7.294686 -11.220618          0\n",
            "256   3.791026 -10.871565          0\n",
            "257   1.683984 -10.469740          0\n",
            "258  -8.632348 -10.509412          0\n",
            "259   7.693656 -10.168918          0\n",
            "260  11.980826 -10.443186          0\n",
            "261   2.482801 -10.422066          0\n",
            "262  -1.137072 -10.439927          0\n",
            "263  -5.928822 -10.242925          0\n",
            "264  -9.568502 -10.228080          0\n",
            "265  10.703390 -10.329122          0\n",
            "266  15.269915 -10.407510          0\n",
            "267   8.487855  -9.811096          0\n",
            "Ticker: MRK\n",
            "Tick Loss: 7.976989793803819\n",
            "        Return        VaR  Indicator\n",
            "255   3.765922 -10.256150          0\n",
            "256  -0.934478 -10.114025          0\n",
            "257  -1.205267 -10.366577          0\n",
            "258  -4.454882  -9.999722          0\n",
            "259   0.890348 -10.050682          0\n",
            "260  18.457777 -10.382297          0\n",
            "261   8.814239 -10.348016          0\n",
            "262   0.753719  -9.957867          0\n",
            "263  -2.549213 -10.266864          0\n",
            "264  -1.089288  -9.919171          0\n",
            "265   0.141189 -10.017398          0\n",
            "266   9.289214 -10.265245          0\n",
            "267  -4.382088 -10.098591          0\n",
            "Ticker: NVO\n",
            "Tick Loss: 7.978279014157834\n",
            "        Return       VaR  Indicator\n",
            "255  -3.157891 -9.140590          0\n",
            "256   0.932963 -9.110447          0\n",
            "257   4.155071 -9.052837          0\n",
            "258  -8.581764 -9.281957          0\n",
            "259  -5.591229 -9.246348          0\n",
            "260   9.244201 -9.248954          0\n",
            "261  14.479974 -9.455308          0\n",
            "262   8.619586 -9.325025          0\n",
            "263   2.541749 -9.352066          0\n",
            "264   1.592445 -9.412651          0\n",
            "265  12.873250 -9.243460          0\n",
            "266   5.836894 -9.362444          0\n",
            "267  -3.967915 -9.356159          0\n",
            "Ticker: PFE\n",
            "Tick Loss: 8.970111785585267\n",
            "        Return        VaR  Indicator\n",
            "255   8.090479 -12.323570          0\n",
            "256  -0.343961 -12.453271          0\n",
            "257  -3.662024 -12.529881          0\n",
            "258  -9.758538 -11.912439          0\n",
            "259  -3.250061 -12.120667          0\n",
            "260   6.375682 -11.822095          0\n",
            "261   7.690670 -11.676204          0\n",
            "262   3.090302 -11.429171          0\n",
            "263 -13.817335 -11.526550          1\n",
            "264  -7.286115 -11.256194          0\n",
            "265   0.566924 -11.483766          0\n",
            "266  -4.681371 -11.293636          0\n",
            "267  -2.237080 -10.981535          0\n",
            "Ticker: ROG\n",
            "Tick Loss: 46.46466835454326\n",
            "        Return        VaR  Indicator\n",
            "255  -1.972516 -17.153891          0\n",
            "256  -1.239735 -17.435919          0\n",
            "257   2.731887 -17.025340          0\n",
            "258  -6.956359 -17.689183          0\n",
            "259  -3.448826 -17.072613          0\n",
            "260  -2.707956 -17.438655          0\n",
            "261 -53.665066 -17.027766          1\n",
            "262   9.446071 -17.164654          0\n",
            "263  16.968326 -17.459701          0\n",
            "264   5.451681 -16.877161          0\n",
            "265  11.025813 -17.386413          0\n",
            "266  -1.517467 -17.639081          0\n",
            "267  -2.162160 -17.223294          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rolling forecast for evaluation"
      ],
      "metadata": {
        "id": "YMJjSm5oDjVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arch\n",
        "import numpy as np\n",
        "\n",
        "def rolling_window_evaluation_GARCH(data, tickers, lagged_covs, lagged_tickers, alpha, t, window_size, horizon):\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_covs_t = [cov for cov in lagged_covs if int(cov.split('_lag')[1]) >= window_size]\n",
        "        lagged_tickers_t_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag') and int(tick.split('_lag')[1]) >= window_size]\n",
        "\n",
        "        for i in range(t, len(data) - window_size - horizon + 1):\n",
        "            # Prepare the data\n",
        "            train_data = data.loc[i: i + window_size - 1]\n",
        "            test_data = data.loc[i + window_size - 1 : i + window_size - 1 + horizon - 1]\n",
        "\n",
        "            X_train = train_data[lagged_covs_t + lagged_tickers_t_ticker]\n",
        "            y_train = train_data[ticker]\n",
        "\n",
        "            garch_model = arch.arch_model(y_train, vol='Garch', p=1, q=1)\n",
        "            garch_results = garch_model.fit()\n",
        "\n",
        "            X_test = test_data[lagged_covs_t + lagged_tickers_t_ticker]\n",
        "            y_test = test_data[ticker]\n",
        "\n",
        "            # Perform forecast\n",
        "            forecasts = garch_results.forecast(start=i + window_size - 1, horizon=horizon)\n",
        "            cond_vol_forecast = forecasts.variance[-1:].apply(lambda x: np.sqrt(x))\n",
        "            mean_forecast = forecasts.mean[-1:]\n",
        "\n",
        "            epsilon = np.random.normal(0, 1, size=(horizon, 10000))\n",
        "            VaR_forecast = mean_forecast.values[0] + cond_vol_forecast.values[0] * np.percentile(epsilon, alpha * 100, axis=1)\n",
        "\n",
        "            loss, df = tick_loss(alpha, y_test, VaR_forecast.flatten())\n",
        "\n",
        "            if ticker not in losses:\n",
        "                losses[ticker] = []\n",
        "\n",
        "            losses[ticker].append((loss, df))\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "w9r5VL2JDonv"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 6\n",
        "horizon = 6"
      ],
      "metadata": {
        "id": "7mdKm2JZDy_y"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the rolling_window_evaluation_GARCH function\n",
        "losses = rolling_window_evaluation_GARCH(data, tickers, lagged_covs, lagged_tickers, alpha, t, window_size, horizon)\n",
        "\n",
        "# Print the losses for each ticker\n",
        "for ticker, ticker_losses in losses.items():\n",
        "    print(f'{ticker}:')\n",
        "    for i, (loss, df) in enumerate(ticker_losses):\n",
        "        print(f'Rolling Window {i+1} - Loss: {loss}, Degrees of Freedom: {df}')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "EofUDgJ2DwT_",
        "outputId": "1b28f5de-fcce-4616-ee89-8f2ae61a8123"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-c204a54e40f0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call the rolling_window_evaluation_GARCH function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling_window_evaluation_GARCH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagged_covs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagged_tickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Print the losses for each ticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker_losses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-7bcfc60ca7ae>\u001b[0m in \u001b[0;36mrolling_window_evaluation_GARCH\u001b[0;34m(data, tickers, lagged_covs, lagged_tickers, alpha, t, window_size, horizon)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlagged_tickers_t_ticker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtick\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlagged_tickers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{ticker}_lag'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_lag'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhorizon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;31m# Prepare the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 months ahead"
      ],
      "metadata": {
        "id": "gGrBpgMM7Bq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tick_loss(alpha, returns, var):\n",
        "    df = pd.DataFrame({'Return': returns, 'VaR': var})\n",
        "    df['Indicator'] = np.where(df['Return'] < df['VaR'], 1, 0)\n",
        "\n",
        "    t_loss = 0\n",
        "\n",
        "    for i in df.index:\n",
        "        t_loss += (\n",
        "            alpha * (df['Return'][i] - df['VaR'][i]) * (1 - df['Indicator'][i])\n",
        "            + (1 - alpha) * (df['VaR'][i] - df['Return'][i]) * df['Indicator'][i]\n",
        "        )\n",
        "\n",
        "    return t_loss, df"
      ],
      "metadata": {
        "id": "YNd_EZxv69l4"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arch\n",
        "import numpy as np\n",
        "\n",
        "def nine_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha, t):\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_covs_t9 = [cov for cov in lagged_covs if int(cov.split('_lag')[1]) >= 9]\n",
        "        lagged_tickers_t9_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag') and int(tick.split('_lag')[1]) >= 9]\n",
        "\n",
        "        # Prepare the data\n",
        "        train_data = data.loc[:t]\n",
        "        test_data = data.loc[t:]\n",
        "\n",
        "        X_train = train_data[lagged_covs_t9 + lagged_tickers_t9_ticker]\n",
        "        y_train = train_data[ticker]\n",
        "\n",
        "        garch_model = arch.arch_model(y_train, vol='Garch', p=1, q=1)\n",
        "        garch_results = garch_model.fit()\n",
        "\n",
        "        X_test = test_data[lagged_covs_t9 + lagged_tickers_t9_ticker]\n",
        "        y_test = test_data[ticker]\n",
        "\n",
        "        # Perform rolling forecast\n",
        "        forecasts = garch_results.forecast(start=t, horizon=len(y_test) + 9)\n",
        "        cond_vol_forecast = forecasts.variance[-1:].apply(lambda x: np.sqrt(x))\n",
        "        mean_forecast = forecasts.mean[-1:]\n",
        "\n",
        "        epsilon = np.random.normal(0, 1, size=(len(y_test) + 9, 10000))\n",
        "        VaR_forecast = mean_forecast.values[0] + cond_vol_forecast.values[0] * np.percentile(epsilon, alpha * 100, axis=1)\n",
        "\n",
        "        loss, df = tick_loss(alpha, y_test, VaR_forecast[-len(y_test):].flatten())\n",
        "\n",
        "        losses[ticker] = (loss, df)\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "uxnlhLgc8-UP"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_nine_step_ahead = nine_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha=0.05, t=255)\n",
        "\n",
        "# Print losses\n",
        "for ticker, loss_tuple in losses_nine_step_ahead.items():\n",
        "    print(f\"Ticker: {ticker}\")\n",
        "    loss, df = loss_tuple\n",
        "    print(f\"Tick Loss: {loss}\")\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfP-ttUE-oYG",
        "outputId": "f4cd68c3-9ddc-461b-901a-28916109edda"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1077.6099789123105\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1213.622379783466\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 872.062964806496\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 770.4995249034941\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 770.4522576366847\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 770.4436016261117\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 770.4431022197208\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 770.4430590287492\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 770.443058525653\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 770.443058525653\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1159.7636996500732\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 849.6317953982542\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 937.2452486616035\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 840.3751966360928\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 840.3670078951413\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 840.3446163506405\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 840.3430701695563\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 840.3430238214773\n",
            "Iteration:      9,   Func. Count:     52,   Neg. LLF: 840.3430238214983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 840.3430238214773\n",
            "            Iterations: 9\n",
            "            Function evaluations: 52\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 876.7003857081925\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 889.3557645435415\n",
            "Iteration:      3,   Func. Count:     19,   Neg. LLF: 872.726750089291\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 857.598346742981\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 857.5943953026263\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 857.5927134533298\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 857.581925754989\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 857.551845666982\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 857.5110447626643\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 857.4737981769581\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 857.4316530780507\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 857.4196825218146\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 857.4191418511459\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 857.419105966716\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 857.4191040831072\n",
            "Iteration:     16,   Func. Count:     85,   Neg. LLF: 857.4191040831308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 857.4191040831072\n",
            "            Iterations: 16\n",
            "            Function evaluations: 85\n",
            "            Gradient evaluations: 16\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 136348755.0961463\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 1128.4906287458045\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 748.6390826665566\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 749.0835703792047\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 747.4560577252767\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 742.0518291214466\n",
            "Iteration:      7,   Func. Count:     45,   Neg. LLF: 741.845826607861\n",
            "Iteration:      8,   Func. Count:     51,   Neg. LLF: 741.8373952071781\n",
            "Iteration:      9,   Func. Count:     57,   Neg. LLF: 741.8365406630396\n",
            "Iteration:     10,   Func. Count:     62,   Neg. LLF: 741.836540077783\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 741.836540077783\n",
            "            Iterations: 10\n",
            "            Function evaluations: 62\n",
            "            Gradient evaluations: 10\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1212.5634737557841\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 897.079383289743\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 853.3850129207542\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 833.5812277154168\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 833.540824940505\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 833.5236201942864\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 833.5213529893995\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 833.5212835725034\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 833.5212826052682\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 833.5212826052682\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 970.8821845361399\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 11769.889291743795\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 852.1121816370958\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 842.1713619359076\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 841.8299414570446\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 841.8199907902888\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 841.8199045567492\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 841.8199040606873\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 841.8199040606873\n",
            "            Iterations: 8\n",
            "            Function evaluations: 49\n",
            "            Gradient evaluations: 8\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1932.3735579222598\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1197.0781359411403\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 857.6135671198253\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 851.4068874978846\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 851.398134712121\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 851.3546243312614\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 851.2924664505352\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 851.2780141089689\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 851.2757031171037\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 851.2749176515492\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 851.2748987022435\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 851.2748886874972\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 851.2748880716983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 851.2748880716983\n",
            "            Iterations: 13\n",
            "            Function evaluations: 71\n",
            "            Gradient evaluations: 13\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1038.1194476046944\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 826.6747012172495\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 1067.7587851808598\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 812.0828012904778\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 811.9997105039965\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 813.3829188680027\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 811.9764948566119\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 811.9674156140086\n",
            "Iteration:      9,   Func. Count:     54,   Neg. LLF: 811.9667545266398\n",
            "Iteration:     10,   Func. Count:     59,   Neg. LLF: 811.9667290699651\n",
            "Iteration:     11,   Func. Count:     64,   Neg. LLF: 811.9667224947991\n",
            "Iteration:     12,   Func. Count:     69,   Neg. LLF: 811.9667219616308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 811.9667219616308\n",
            "            Iterations: 12\n",
            "            Function evaluations: 69\n",
            "            Gradient evaluations: 12\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1209.1403758137285\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 986.7696941746776\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 984.8933363378974\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 984.8920062555766\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 984.8919876942392\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 984.8919726042732\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 984.8919010711488\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 984.8917390018631\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 984.8913204943328\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 984.8902787142376\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 984.8878495712363\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 984.8834254476269\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 984.8782829269476\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 984.8762848574709\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 984.875906761326\n",
            "Iteration:     16,   Func. Count:     86,   Neg. LLF: 984.8758842422201\n",
            "Iteration:     17,   Func. Count:     90,   Neg. LLF: 984.8758842423346\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 984.8758842422201\n",
            "            Iterations: 17\n",
            "            Function evaluations: 90\n",
            "            Gradient evaluations: 17\n",
            "Ticker: NVS\n",
            "Tick Loss: 5.571496100068985\n",
            "        Return       VaR  Indicator\n",
            "255   3.305696 -7.314039          0\n",
            "256  -7.048606 -7.191520          0\n",
            "257   1.537918 -7.267722          0\n",
            "258  -6.186653 -7.228227          0\n",
            "259  -5.601082 -7.260630          0\n",
            "260   6.735944 -7.186229          0\n",
            "261  10.390734 -7.299994          0\n",
            "262   1.295221 -7.280222          0\n",
            "263  -0.110227 -7.320090          0\n",
            "264  -7.172811 -7.424870          0\n",
            "265   9.367574 -7.331468          0\n",
            "266  16.334413 -7.246932          0\n",
            "267  -6.161645 -7.391504          0\n",
            "Ticker: AZN\n",
            "Tick Loss: 8.76730970177839\n",
            "        Return       VaR  Indicator\n",
            "255   0.120491 -9.536108          0\n",
            "256  -0.616736 -9.713261          0\n",
            "257   0.242173 -9.720444          0\n",
            "258  -5.813079 -9.989214          0\n",
            "259 -11.466275 -9.735440          1\n",
            "260   7.239243 -9.645838          0\n",
            "261  15.575584 -9.760341          0\n",
            "262  -0.250105 -9.518175          0\n",
            "263  -3.584079 -9.479470          0\n",
            "264  -0.290649 -9.895393          0\n",
            "265   8.035329 -9.827232          0\n",
            "266   5.489119 -9.664947          0\n",
            "267  -0.191204 -9.753827          0\n",
            "Ticker: BMY\n",
            "Tick Loss: 6.3654360756428\n",
            "        Return        VaR  Indicator\n",
            "255   0.239147 -10.712482          0\n",
            "256   2.054343 -10.462150          0\n",
            "257  -3.516240 -10.648525          0\n",
            "258  -8.633766 -10.464638          0\n",
            "259   5.459121 -10.585528          0\n",
            "260   8.974528 -10.547529          0\n",
            "261   4.416367 -10.565880          0\n",
            "262 -10.376181 -10.701788          0\n",
            "263   0.972908 -10.585152          0\n",
            "264  -4.328217 -10.940704          0\n",
            "265   0.507544 -10.555168          0\n",
            "266  -3.664707 -10.724721          0\n",
            "267  -2.695194 -10.404804          0\n",
            "Ticker: JNJ\n",
            "Tick Loss: 6.1655892000879975\n",
            "       Return       VaR  Indicator\n",
            "255 -0.515348 -6.532574          0\n",
            "256 -0.489803 -6.423469          0\n",
            "257 -1.684398 -6.493121          0\n",
            "258 -7.552151 -6.298519          1\n",
            "259  1.932340 -6.320263          0\n",
            "260  6.494844 -6.327228          0\n",
            "261  2.316498 -6.347270          0\n",
            "262 -0.117873 -6.297434          0\n",
            "263 -7.489384 -6.131840          1\n",
            "264 -6.217115 -6.213883          1\n",
            "265  1.862736 -6.304400          0\n",
            "266  5.612908 -6.172199          0\n",
            "267 -5.277949 -6.286128          0\n",
            "Ticker: LLY\n",
            "Tick Loss: 8.884213618061768\n",
            "        Return        VaR  Indicator\n",
            "255   7.294686 -10.782287          0\n",
            "256   3.791026 -10.295727          0\n",
            "257   1.683984 -10.371769          0\n",
            "258  -8.632348 -10.181361          0\n",
            "259   7.693656 -10.588923          0\n",
            "260  11.980826 -10.389222          0\n",
            "261   2.482801 -10.355869          0\n",
            "262  -1.137072 -10.077621          0\n",
            "263  -5.928822 -10.069093          0\n",
            "264  -9.568502 -10.145097          0\n",
            "265  10.703390 -10.156194          0\n",
            "266  15.269915 -10.110998          0\n",
            "267   8.487855 -10.038718          0\n",
            "Ticker: MRK\n",
            "Tick Loss: 7.951053863932183\n",
            "        Return        VaR  Indicator\n",
            "255   3.765922 -10.279625          0\n",
            "256  -0.934478 -10.049146          0\n",
            "257  -1.205267 -10.252836          0\n",
            "258  -4.454882 -10.268275          0\n",
            "259   0.890348  -9.947401          0\n",
            "260  18.457777 -10.188092          0\n",
            "261   8.814239 -10.070086          0\n",
            "262   0.753719  -9.837950          0\n",
            "263  -2.549213 -10.004166          0\n",
            "264  -1.089288 -10.139057          0\n",
            "265   0.141189 -10.325933          0\n",
            "266   9.289214 -10.105674          0\n",
            "267  -4.382088 -10.055644          0\n",
            "Ticker: NVO\n",
            "Tick Loss: 8.031198138069007\n",
            "        Return       VaR  Indicator\n",
            "255  -3.157891 -9.317313          0\n",
            "256   0.932963 -9.211257          0\n",
            "257   4.155071 -9.431484          0\n",
            "258  -8.581764 -9.184696          0\n",
            "259  -5.591229 -9.521737          0\n",
            "260   9.244201 -9.252932          0\n",
            "261  14.479974 -9.413675          0\n",
            "262   8.619586 -9.248516          0\n",
            "263   2.541749 -9.463599          0\n",
            "264   1.592445 -9.278631          0\n",
            "265  12.873250 -9.239920          0\n",
            "266   5.836894 -9.603263          0\n",
            "267  -3.967915 -9.479604          0\n",
            "Ticker: PFE\n",
            "Tick Loss: 9.052168764737926\n",
            "        Return        VaR  Indicator\n",
            "255   8.090479 -12.283210          0\n",
            "256  -0.343961 -11.913563          0\n",
            "257  -3.662024 -11.725155          0\n",
            "258  -9.758538 -11.751896          0\n",
            "259  -3.250061 -11.448385          0\n",
            "260   6.375682 -11.711779          0\n",
            "261   7.690670 -11.850741          0\n",
            "262   3.090302 -11.355988          0\n",
            "263 -13.817335 -11.294958          1\n",
            "264  -7.286115 -11.242176          0\n",
            "265   0.566924 -11.333544          0\n",
            "266  -4.681371 -11.115510          0\n",
            "267  -2.237080 -10.791374          0\n",
            "Ticker: ROG\n",
            "Tick Loss: 46.11865066247204\n",
            "        Return        VaR  Indicator\n",
            "255  -1.972516 -17.318317          0\n",
            "256  -1.239735 -16.928761          0\n",
            "257   2.731887 -17.373329          0\n",
            "258  -6.956359 -17.658731          0\n",
            "259  -3.448826 -17.376513          0\n",
            "260  -2.707956 -17.618707          0\n",
            "261 -53.665066 -17.437310          1\n",
            "262   9.446071 -17.184342          0\n",
            "263  16.968326 -17.837054          0\n",
            "264   5.451681 -17.722435          0\n",
            "265  11.025813 -16.820577          0\n",
            "266  -1.517467 -17.235812          0\n",
            "267  -2.162160 -17.352304          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12 months ahead"
      ],
      "metadata": {
        "id": "qXdurz7P_HMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tick_loss(alpha, returns, var):\n",
        "    df = pd.DataFrame({'Return': returns, 'VaR': var})\n",
        "    df['Indicator'] = np.where(df['Return'] < df['VaR'], 1, 0)\n",
        "\n",
        "    t_loss = 0\n",
        "\n",
        "    for i in df.index:\n",
        "        t_loss += (\n",
        "            alpha * (df['Return'][i] - df['VaR'][i]) * (1 - df['Indicator'][i])\n",
        "            + (1 - alpha) * (df['VaR'][i] - df['Return'][i]) * df['Indicator'][i]\n",
        "        )\n",
        "\n",
        "    return t_loss, df"
      ],
      "metadata": {
        "id": "5ic2anFr-oRu"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arch\n",
        "import numpy as np\n",
        "\n",
        "def twelve_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha, t):\n",
        "    losses = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        lagged_covs_t12 = [cov for cov in lagged_covs if int(cov.split('_lag')[1]) >= 12]\n",
        "        lagged_tickers_t12_ticker = [tick for tick in lagged_tickers if tick.startswith(f'{ticker}_lag') and int(tick.split('_lag')[1]) >= 12]\n",
        "\n",
        "        # Prepare the data\n",
        "        train_data = data.loc[:t]\n",
        "        test_data = data.loc[t:]\n",
        "\n",
        "        X_train = train_data[lagged_covs_t12 + lagged_tickers_t12_ticker]\n",
        "        y_train = train_data[ticker]\n",
        "\n",
        "        garch_model = arch.arch_model(y_train, vol='Garch', p=1, q=1)\n",
        "        garch_results = garch_model.fit()\n",
        "\n",
        "        X_test = test_data[lagged_covs_t12 + lagged_tickers_t12_ticker]\n",
        "        y_test = test_data[ticker]\n",
        "\n",
        "        # Perform rolling forecast\n",
        "        forecasts = garch_results.forecast(start=t, horizon=len(y_test) + 12)\n",
        "        cond_vol_forecast = forecasts.variance[-1:].apply(lambda x: np.sqrt(x))\n",
        "        mean_forecast = forecasts.mean[-1:]\n",
        "\n",
        "        epsilon = np.random.normal(0, 1, size=(len(y_test) + 12, 10000))\n",
        "        VaR_forecast = mean_forecast.values[0] + cond_vol_forecast.values[0] * np.percentile(epsilon, alpha * 100, axis=1)\n",
        "\n",
        "        loss, df = tick_loss(alpha, y_test, VaR_forecast[-len(y_test):].flatten())\n",
        "\n",
        "        losses[ticker] = (loss, df)\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "7yMutSjC_Ny5"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_twelve_step_ahead = twelve_step_ahead_VaR(data, tickers, lagged_covs, lagged_tickers, alpha=0.05, t=255)\n",
        "\n",
        "# Print losses\n",
        "for ticker, loss_tuple in losses_twelve_step_ahead.items():\n",
        "    print(f\"Ticker: {ticker}\")\n",
        "    loss, df = loss_tuple\n",
        "    print(f\"Tick Loss: {loss}\")\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJVcSFD0_wDc",
        "outputId": "8cc4defd-e62e-4bef-d687-ceab73ed2256"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1077.6099789123105\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1213.622379783466\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 872.062964806496\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 770.4995249034941\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 770.4522576366847\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 770.4436016261117\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 770.4431022197208\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 770.4430590287492\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 770.443058525653\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 770.443058525653\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1159.7636996500732\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 849.6317953982542\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 937.2452486616035\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 840.3751966360928\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 840.3670078951413\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 840.3446163506405\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 840.3430701695563\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 840.3430238214773\n",
            "Iteration:      9,   Func. Count:     52,   Neg. LLF: 840.3430238214983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 840.3430238214773\n",
            "            Iterations: 9\n",
            "            Function evaluations: 52\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 876.7003857081925\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 889.3557645435415\n",
            "Iteration:      3,   Func. Count:     19,   Neg. LLF: 872.726750089291\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 857.598346742981\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 857.5943953026263\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 857.5927134533298\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 857.581925754989\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 857.551845666982\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 857.5110447626643\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 857.4737981769581\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 857.4316530780507\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 857.4196825218146\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 857.4191418511459\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 857.419105966716\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 857.4191040831072\n",
            "Iteration:     16,   Func. Count:     85,   Neg. LLF: 857.4191040831308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 857.4191040831072\n",
            "            Iterations: 16\n",
            "            Function evaluations: 85\n",
            "            Gradient evaluations: 16\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 136348755.0961463\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 1128.4906287458045\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 748.6390826665566\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 749.0835703792047\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 747.4560577252767\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 742.0518291214466\n",
            "Iteration:      7,   Func. Count:     45,   Neg. LLF: 741.845826607861\n",
            "Iteration:      8,   Func. Count:     51,   Neg. LLF: 741.8373952071781\n",
            "Iteration:      9,   Func. Count:     57,   Neg. LLF: 741.8365406630396\n",
            "Iteration:     10,   Func. Count:     62,   Neg. LLF: 741.836540077783\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 741.836540077783\n",
            "            Iterations: 10\n",
            "            Function evaluations: 62\n",
            "            Gradient evaluations: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1212.5634737557841\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 897.079383289743\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 853.3850129207542\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 833.5812277154168\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 833.540824940505\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 833.5236201942864\n",
            "Iteration:      7,   Func. Count:     43,   Neg. LLF: 833.5213529893995\n",
            "Iteration:      8,   Func. Count:     48,   Neg. LLF: 833.5212835725034\n",
            "Iteration:      9,   Func. Count:     53,   Neg. LLF: 833.5212826052682\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 833.5212826052682\n",
            "            Iterations: 9\n",
            "            Function evaluations: 53\n",
            "            Gradient evaluations: 9\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 970.8821845361399\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 11769.889291743795\n",
            "Iteration:      3,   Func. Count:     21,   Neg. LLF: 852.1121816370958\n",
            "Iteration:      4,   Func. Count:     27,   Neg. LLF: 842.1713619359076\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 841.8299414570446\n",
            "Iteration:      6,   Func. Count:     39,   Neg. LLF: 841.8199907902888\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 841.8199045567492\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 841.8199040606873\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 841.8199040606873\n",
            "            Iterations: 8\n",
            "            Function evaluations: 49\n",
            "            Gradient evaluations: 8\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1932.3735579222598\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 1197.0781359411403\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 857.6135671198253\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 851.4068874978846\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 851.398134712121\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 851.3546243312614\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 851.2924664505352\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 851.2780141089689\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 851.2757031171037\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 851.2749176515492\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 851.2748987022435\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 851.2748886874972\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 851.2748880716983\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 851.2748880716983\n",
            "            Iterations: 13\n",
            "            Function evaluations: 71\n",
            "            Gradient evaluations: 13\n",
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1038.1194476046944\n",
            "Iteration:      2,   Func. Count:     13,   Neg. LLF: 826.6747012172495\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 1067.7587851808598\n",
            "Iteration:      4,   Func. Count:     28,   Neg. LLF: 812.0828012904778\n",
            "Iteration:      5,   Func. Count:     33,   Neg. LLF: 811.9997105039965\n",
            "Iteration:      6,   Func. Count:     38,   Neg. LLF: 813.3829188680027\n",
            "Iteration:      7,   Func. Count:     44,   Neg. LLF: 811.9764948566119\n",
            "Iteration:      8,   Func. Count:     49,   Neg. LLF: 811.9674156140086\n",
            "Iteration:      9,   Func. Count:     54,   Neg. LLF: 811.9667545266398\n",
            "Iteration:     10,   Func. Count:     59,   Neg. LLF: 811.9667290699651\n",
            "Iteration:     11,   Func. Count:     64,   Neg. LLF: 811.9667224947991\n",
            "Iteration:     12,   Func. Count:     69,   Neg. LLF: 811.9667219616308\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 811.9667219616308\n",
            "            Iterations: 12\n",
            "            Function evaluations: 69\n",
            "            Gradient evaluations: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:      1,   Func. Count:      6,   Neg. LLF: 1209.1403758137285\n",
            "Iteration:      2,   Func. Count:     14,   Neg. LLF: 986.7696941746776\n",
            "Iteration:      3,   Func. Count:     20,   Neg. LLF: 984.8933363378974\n",
            "Iteration:      4,   Func. Count:     26,   Neg. LLF: 984.8920062555766\n",
            "Iteration:      5,   Func. Count:     31,   Neg. LLF: 984.8919876942392\n",
            "Iteration:      6,   Func. Count:     36,   Neg. LLF: 984.8919726042732\n",
            "Iteration:      7,   Func. Count:     41,   Neg. LLF: 984.8919010711488\n",
            "Iteration:      8,   Func. Count:     46,   Neg. LLF: 984.8917390018631\n",
            "Iteration:      9,   Func. Count:     51,   Neg. LLF: 984.8913204943328\n",
            "Iteration:     10,   Func. Count:     56,   Neg. LLF: 984.8902787142376\n",
            "Iteration:     11,   Func. Count:     61,   Neg. LLF: 984.8878495712363\n",
            "Iteration:     12,   Func. Count:     66,   Neg. LLF: 984.8834254476269\n",
            "Iteration:     13,   Func. Count:     71,   Neg. LLF: 984.8782829269476\n",
            "Iteration:     14,   Func. Count:     76,   Neg. LLF: 984.8762848574709\n",
            "Iteration:     15,   Func. Count:     81,   Neg. LLF: 984.875906761326\n",
            "Iteration:     16,   Func. Count:     86,   Neg. LLF: 984.8758842422201\n",
            "Iteration:     17,   Func. Count:     90,   Neg. LLF: 984.8758842423346\n",
            "Optimization terminated successfully    (Exit mode 0)\n",
            "            Current function value: 984.8758842422201\n",
            "            Iterations: 17\n",
            "            Function evaluations: 90\n",
            "            Gradient evaluations: 17\n",
            "Ticker: NVS\n",
            "Tick Loss: 5.5686716103101235\n",
            "        Return       VaR  Indicator\n",
            "255   3.305696 -7.268155          0\n",
            "256  -7.048606 -7.383900          0\n",
            "257   1.537918 -7.357450          0\n",
            "258  -6.186653 -7.506811          0\n",
            "259  -5.601082 -7.338503          0\n",
            "260   6.735944 -7.087364          0\n",
            "261  10.390734 -7.201786          0\n",
            "262   1.295221 -7.183330          0\n",
            "263  -0.110227 -7.263134          0\n",
            "264  -7.172811 -7.364220          0\n",
            "265   9.367574 -7.366056          0\n",
            "266  16.334413 -7.194458          0\n",
            "267  -6.161645 -7.171789          0\n",
            "Ticker: AZN\n",
            "Tick Loss: 8.852179005794017\n",
            "        Return        VaR  Indicator\n",
            "255   0.120491  -9.695187          0\n",
            "256  -0.616736  -9.573844          0\n",
            "257   0.242173  -9.746923          0\n",
            "258  -5.813079 -10.023071          0\n",
            "259 -11.466275  -9.674984          1\n",
            "260   7.239243  -9.693548          0\n",
            "261  15.575584  -9.852027          0\n",
            "262  -0.250105  -9.819045          0\n",
            "263  -3.584079  -9.779979          0\n",
            "264  -0.290649  -9.825123          0\n",
            "265   8.035329  -9.648356          0\n",
            "266   5.489119  -9.591878          0\n",
            "267  -0.191204  -9.803983          0\n",
            "Ticker: BMY\n",
            "Tick Loss: 6.388315094544258\n",
            "        Return        VaR  Indicator\n",
            "255   0.239147 -10.646561          0\n",
            "256   2.054343 -10.515410          0\n",
            "257  -3.516240 -10.415784          0\n",
            "258  -8.633766 -10.807614          0\n",
            "259   5.459121 -10.724274          0\n",
            "260   8.974528 -10.762641          0\n",
            "261   4.416367 -10.794439          0\n",
            "262 -10.376181 -10.455499          0\n",
            "263   0.972908 -10.710940          0\n",
            "264  -4.328217 -10.522081          0\n",
            "265   0.507544 -10.753563          0\n",
            "266  -3.664707 -10.588203          0\n",
            "267  -2.695194 -10.659640          0\n",
            "Ticker: JNJ\n",
            "Tick Loss: 5.813395478461864\n",
            "       Return       VaR  Indicator\n",
            "255 -0.515348 -6.465851          0\n",
            "256 -0.489803 -6.352616          0\n",
            "257 -1.684398 -6.284943          0\n",
            "258 -7.552151 -6.328847          1\n",
            "259  1.932340 -6.380102          0\n",
            "260  6.494844 -6.336217          0\n",
            "261  2.316498 -6.367339          0\n",
            "262 -0.117873 -6.419948          0\n",
            "263 -7.489384 -6.483094          1\n",
            "264 -6.217115 -6.331976          0\n",
            "265  1.862736 -6.286055          0\n",
            "266  5.612908 -6.321442          0\n",
            "267 -5.277949 -6.442289          0\n",
            "Ticker: LLY\n",
            "Tick Loss: 8.787061735564444\n",
            "        Return        VaR  Indicator\n",
            "255   7.294686 -10.467485          0\n",
            "256   3.791026 -10.275272          0\n",
            "257   1.683984 -10.459611          0\n",
            "258  -8.632348 -10.401370          0\n",
            "259   7.693656 -10.232440          0\n",
            "260  11.980826 -10.204184          0\n",
            "261   2.482801 -10.077777          0\n",
            "262  -1.137072 -10.036708          0\n",
            "263  -5.928822 -10.088313          0\n",
            "264  -9.568502  -9.769618          0\n",
            "265  10.703390  -9.889194          0\n",
            "266  15.269915  -9.840990          0\n",
            "267   8.487855  -9.876879          0\n",
            "Ticker: MRK\n",
            "Tick Loss: 7.923665281384387\n",
            "        Return        VaR  Indicator\n",
            "255   3.765922  -9.887031          0\n",
            "256  -0.934478  -9.990314          0\n",
            "257  -1.205267 -10.263706          0\n",
            "258  -4.454882 -10.121918          0\n",
            "259   0.890348  -9.953121          0\n",
            "260  18.457777  -9.951008          0\n",
            "261   8.814239 -10.334336          0\n",
            "262   0.753719 -10.044752          0\n",
            "263  -2.549213 -10.068468          0\n",
            "264  -1.089288  -9.922392          0\n",
            "265   0.141189 -10.113255          0\n",
            "266   9.289214 -10.221233          0\n",
            "267  -4.382088 -10.104580          0\n",
            "Ticker: NVO\n",
            "Tick Loss: 8.04992922846629\n",
            "        Return       VaR  Indicator\n",
            "255  -3.157891 -9.355783          0\n",
            "256   0.932963 -9.392344          0\n",
            "257   4.155071 -9.495164          0\n",
            "258  -8.581764 -9.228032          0\n",
            "259  -5.591229 -9.508857          0\n",
            "260   9.244201 -9.336398          0\n",
            "261  14.479974 -9.345943          0\n",
            "262   8.619586 -9.299013          0\n",
            "263   2.541749 -9.397261          0\n",
            "264   1.592445 -9.400601          0\n",
            "265  12.873250 -9.315985          0\n",
            "266   5.836894 -9.371896          0\n",
            "267  -3.967915 -9.573973          0\n",
            "Ticker: PFE\n",
            "Tick Loss: 9.246853893851949\n",
            "        Return        VaR  Indicator\n",
            "255   8.090479 -11.652025          0\n",
            "256  -0.343961 -11.636330          0\n",
            "257  -3.662024 -11.603900          0\n",
            "258  -9.758538 -11.494147          0\n",
            "259  -3.250061 -11.540346          0\n",
            "260   6.375682 -11.445095          0\n",
            "261   7.690670 -11.155900          0\n",
            "262   3.090302 -10.967300          0\n",
            "263 -13.817335 -10.902141          1\n",
            "264  -7.286115 -11.065691          0\n",
            "265   0.566924 -10.952926          0\n",
            "266  -4.681371 -10.785915          0\n",
            "267  -2.237080 -10.653918          0\n",
            "Ticker: ROG\n",
            "Tick Loss: 46.13454232449846\n",
            "        Return        VaR  Indicator\n",
            "255  -1.972516 -17.483366          0\n",
            "256  -1.239735 -18.049090          0\n",
            "257   2.731887 -17.699858          0\n",
            "258  -6.956359 -17.394115          0\n",
            "259  -3.448826 -17.477643          0\n",
            "260  -2.707956 -17.745483          0\n",
            "261 -53.665066 -17.526161          1\n",
            "262   9.446071 -17.369797          0\n",
            "263  16.968326 -17.463919          0\n",
            "264   5.451681 -17.442899          0\n",
            "265  11.025813 -17.090831          0\n",
            "266  -1.517467 -17.586865          0\n",
            "267  -2.162160 -17.629027          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
            "The default for reindex is True. After September 2021 this will change to\n",
            "False. Set reindex to True or False to silence this message. Alternatively,\n",
            "you can use the import comment\n",
            "\n",
            "from arch.__future__ import reindexing\n",
            "\n",
            "to globally set reindex to True and silence this warning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL COMPARISON"
      ],
      "metadata": {
        "id": "iqwPI4-HAShu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UExeDjq-AUgF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}